{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5aa91b1",
   "metadata": {},
   "source": [
    "# Machine Learning Models for Loan Default Prediction\n",
    "## COMP647 Assignment 03\n",
    "### Student ID: 1163127\n",
    "\n",
    "This notebook implements supervised learning classification algorithms:\n",
    "- Logistic Regression\n",
    "- Random Forest Classifier\n",
    "- Gradient Boosting Classifier\n",
    "\n",
    "Each model includes:\n",
    "- Hyperparameter tuning justification\n",
    "- Training and validation\n",
    "- Performance comparison\n",
    "\n",
    "Based on course materials and ML best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    ")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "try:\n",
    "    df = pd.read_csv('../data/processed/accepted_sample_10000.csv')\n",
    "    print(f\"Data loaded: {df.shape}\")\n",
    "    \n",
    "    # Identify target\n",
    "    target_column = 'loan_status'\n",
    "    \n",
    "    if target_column in df.columns:\n",
    "        print(f\"\\nTarget variable: {target_column}\")\n",
    "        print(df[target_column].value_counts())\n",
    "    else:\n",
    "        print(\"Target column not found\")\n",
    "        target_column = None\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Data not found\")\n",
    "    df = pd.DataFrame()\n",
    "    target_column = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8f65d",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Prepare features and target for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "if not df.empty and target_column:\n",
    "    # Select numerical features\n",
    "    numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numerical_features = [c for c in numerical_features \n",
    "                         if c != target_column and not c.endswith('_id')]\n",
    "    \n",
    "    # Limit to top features for efficiency (use top 15)\n",
    "    if len(numerical_features) > 15:\n",
    "        # Use correlation with target to select top features\n",
    "        X_temp = df[numerical_features].fillna(df[numerical_features].mean())\n",
    "        le = LabelEncoder()\n",
    "        y_temp = le.fit_transform(df[target_column].fillna('Unknown'))\n",
    "        \n",
    "        correlations = []\n",
    "        for col in numerical_features:\n",
    "            corr = abs(np.corrcoef(X_temp[col], y_temp)[0, 1])\n",
    "            correlations.append((col, corr))\n",
    "        \n",
    "        correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected_features = [c[0] for c in correlations[:15]]\n",
    "    else:\n",
    "        selected_features = numerical_features\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features for modeling\")\n",
    "    print(\"Features:\", selected_features[:10], \"...\")\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = df[selected_features].fillna(df[selected_features].mean())\n",
    "    \n",
    "    # Encode target to binary\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[target_column].fillna('Unknown'))\n",
    "    \n",
    "    # Convert to binary if multi-class\n",
    "    if len(np.unique(y)) > 2:\n",
    "        # Use most common class vs others\n",
    "        most_common = pd.Series(y).mode()[0]\n",
    "        y = (y == most_common).astype(int)\n",
    "        print(f\"\\nConverted to binary: class {most_common} vs others\")\n",
    "    \n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(pd.Series(y).value_counts())\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"\\nFeatures prepared and scaled successfully\")\n",
    "else:\n",
    "    print(\"Data preparation failed\")\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b86d2",
   "metadata": {},
   "source": [
    "## Model 1: Logistic Regression\n",
    "Linear model suitable for binary classification with interpretable coefficients.\n",
    "\n",
    "**Hyperparameters:**\n",
    "- C (regularization): Controls model complexity\n",
    "- solver: Algorithm for optimization\n",
    "- max_iter: Maximum iterations for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccec010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "if X_train is not None:\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    print(\"\\nHyperparameter justification:\")\n",
    "    print(\"- C=1.0: Balanced regularization (not too strict, not too loose)\")\n",
    "    print(\"- solver='liblinear': Good for small datasets\")\n",
    "    print(\"- max_iter=1000: Ensures convergence\")\n",
    "    \n",
    "    lr_model = LogisticRegression(\n",
    "        C=1.0,\n",
    "        solver='liblinear',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    lr_train_pred = lr_model.predict(X_train_scaled)\n",
    "    lr_test_pred = lr_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    print(\"\\n--- Training Performance ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, lr_train_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_train, lr_train_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_train, lr_train_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_train, lr_train_pred):.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Test Performance ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, lr_test_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, lr_test_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, lr_test_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, lr_test_pred):.4f}\")\n",
    "    \n",
    "    print(\"\\nLogistic Regression training complete\")\n",
    "else:\n",
    "    print(\"Cannot train - data not prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01545d",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest Classifier\n",
    "Ensemble method using multiple decision trees, robust to overfitting.\n",
    "\n",
    "**Hyperparameters:**\n",
    "- n_estimators: Number of trees\n",
    "- max_depth: Tree depth limit\n",
    "- min_samples_split: Minimum samples to split node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37590c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "if X_train is not None:\n",
    "    print(\"Training Random Forest...\")\n",
    "    print(\"\\nHyperparameter justification:\")\n",
    "    print(\"- n_estimators=100: Balance between performance and speed\")\n",
    "    print(\"- max_depth=10: Prevents overfitting while capturing patterns\")\n",
    "    print(\"- min_samples_split=20: Avoids overly specific splits\")\n",
    "    \n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)  # RF doesn't require scaling\n",
    "    \n",
    "    # Predictions\n",
    "    rf_train_pred = rf_model.predict(X_train)\n",
    "    rf_test_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    print(\"\\n--- Training Performance ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, rf_train_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_train, rf_train_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_train, rf_train_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_train, rf_train_pred):.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Test Performance ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, rf_test_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, rf_test_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, rf_test_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, rf_test_pred):.4f}\")\n",
    "    \n",
    "    print(\"\\nRandom Forest training complete\")\n",
    "else:\n",
    "    print(\"Cannot train - data not prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7c4d2",
   "metadata": {},
   "source": [
    "## Model 3: Gradient Boosting Classifier\n",
    "Sequential ensemble method that builds trees to correct previous errors.\n",
    "\n",
    "**Hyperparameters:**\n",
    "- n_estimators: Number of boosting stages\n",
    "- learning_rate: Shrinkage rate\n",
    "- max_depth: Individual tree depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b725625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting\n",
    "if X_train is not None:\n",
    "    print(\"Training Gradient Boosting...\")\n",
    "    print(\"\\nHyperparameter justification:\")\n",
    "    print(\"- n_estimators=100: Sufficient boosting rounds\")\n",
    "    print(\"- learning_rate=0.1: Standard rate for stable learning\")\n",
    "    print(\"- max_depth=3: Shallow trees prevent overfitting in boosting\")\n",
    "    \n",
    "    gb_model = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    gb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    gb_train_pred = gb_model.predict(X_train)\n",
    "    gb_test_pred = gb_model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    print(\"\\n--- Training Performance ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, gb_train_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_train, gb_train_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_train, gb_train_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_train, gb_train_pred):.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Test Performance ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, gb_test_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, gb_test_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, gb_test_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, gb_test_pred):.4f}\")\n",
    "    \n",
    "    print(\"\\nGradient Boosting training complete\")\n",
    "else:\n",
    "    print(\"Cannot train - data not prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828b62b",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "Compare performance of all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc5b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "if X_train is not None:\n",
    "    models_comparison = pd.DataFrame({\n",
    "        'Model': ['Logistic Regression', 'Random Forest', 'Gradient Boosting'],\n",
    "        'Train_Accuracy': [\n",
    "            accuracy_score(y_train, lr_train_pred),\n",
    "            accuracy_score(y_train, rf_train_pred),\n",
    "            accuracy_score(y_train, gb_train_pred)\n",
    "        ],\n",
    "        'Test_Accuracy': [\n",
    "            accuracy_score(y_test, lr_test_pred),\n",
    "            accuracy_score(y_test, rf_test_pred),\n",
    "            accuracy_score(y_test, gb_test_pred)\n",
    "        ],\n",
    "        'Test_Precision': [\n",
    "            precision_score(y_test, lr_test_pred),\n",
    "            precision_score(y_test, rf_test_pred),\n",
    "            precision_score(y_test, gb_test_pred)\n",
    "        ],\n",
    "        'Test_Recall': [\n",
    "            recall_score(y_test, lr_test_pred),\n",
    "            recall_score(y_test, rf_test_pred),\n",
    "            recall_score(y_test, gb_test_pred)\n",
    "        ],\n",
    "        'Test_F1': [\n",
    "            f1_score(y_test, lr_test_pred),\n",
    "            f1_score(y_test, rf_test_pred),\n",
    "            f1_score(y_test, gb_test_pred)\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"=== Model Performance Comparison ===\\n\")\n",
    "    print(models_comparison.to_string(index=False))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Test metrics comparison\n",
    "    metrics = ['Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1']\n",
    "    models_comparison.set_index('Model')[metrics].plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('Test Set Performance Metrics')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Train vs Test accuracy (overfitting check)\n",
    "    x = np.arange(len(models_comparison))\n",
    "    width = 0.35\n",
    "    axes[1].bar(x - width/2, models_comparison['Train_Accuracy'], width, label='Train')\n",
    "    axes[1].bar(x + width/2, models_comparison['Test_Accuracy'], width, label='Test')\n",
    "    axes[1].set_xlabel('Model')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Train vs Test Accuracy (Overfitting Check)')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(models_comparison['Model'], rotation=15, ha='right')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n=== Recommendations ===\")\n",
    "    best_model = models_comparison.loc[models_comparison['Test_F1'].idxmax(), 'Model']\n",
    "    print(f\"Best model by F1-score: {best_model}\")\n",
    "    \n",
    "    # Check overfitting\n",
    "    for idx, row in models_comparison.iterrows():\n",
    "        gap = row['Train_Accuracy'] - row['Test_Accuracy']\n",
    "        if gap > 0.1:\n",
    "            print(f\"{row['Model']}: Shows overfitting (train-test gap = {gap:.3f})\")\n",
    "        elif gap < 0.05:\n",
    "            print(f\"{row['Model']}: Well-generalized model\")\n",
    "else:\n",
    "    print(\"Cannot compare - models not trained\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
