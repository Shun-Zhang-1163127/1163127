{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1b2a0e",
   "metadata": {},
   "source": [
    "# Model Evaluation and Validation\n",
    "## COMP647 Assignment 03\n",
    "### Student ID: 1163127\n",
    "\n",
    "This notebook implements comprehensive model evaluation:\n",
    "- Detailed performance metrics (Accuracy, Precision, Recall, F1)\n",
    "- Confusion matrices\n",
    "- ROC curves and AUC scores\n",
    "- K-fold cross-validation\n",
    "- Learning curves for overfitting analysis\n",
    "\n",
    "Based on course materials and ML evaluation best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e002a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, learning_curve, validation_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, \n",
    "    roc_curve, roc_auc_score, plot_confusion_matrix\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749cd66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data (same as notebook 06)\n",
    "try:\n",
    "    df = pd.read_csv('../data/processed/accepted_sample_10000.csv')\n",
    "    print(f\"Data loaded: {df.shape}\")\n",
    "    \n",
    "    target_column = 'loan_status'\n",
    "    \n",
    "    # Select numerical features\n",
    "    numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numerical_features = [c for c in numerical_features \n",
    "                         if c != target_column and not c.endswith('_id')][:15]\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = df[numerical_features].fillna(df[numerical_features].mean())\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[target_column].fillna('Unknown'))\n",
    "    \n",
    "    if len(np.unique(y)) > 2:\n",
    "        y = (y == pd.Series(y).mode()[0]).astype(int)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    print(\"Data prepared successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeae2eb",
   "metadata": {},
   "source": [
    "## Confusion Matrices\n",
    "Detailed breakdown of classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models and create confusion matrices\n",
    "if X_train is not None:\n",
    "    # Train models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(C=1.0, solver='liblinear', random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "    }\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for idx, (name, model) in enumerate(models.items()):\n",
    "        # Train\n",
    "        if 'Logistic' in name:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        predictions[name] = y_pred\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Plot\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "        axes[idx].set_title(f'{name}\\nConfusion Matrix')\n",
    "        axes[idx].set_ylabel('True Label')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Confusion matrices generated\")\n",
    "else:\n",
    "    print(\"Cannot generate - data not prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3f8ad",
   "metadata": {},
   "source": [
    "## ROC Curves and AUC Scores\n",
    "Receiver Operating Characteristic curves show trade-off between TPR and FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac134de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC curves\n",
    "if X_train is not None:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Get probability predictions\n",
    "        if 'Logistic' in name:\n",
    "            y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        \n",
    "        # Plot\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)\n",
    "    \n",
    "    # Diagonal line (random classifier)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for All Models')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ROC curves generated\")\n",
    "else:\n",
    "    print(\"Cannot generate - data not prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c7bbcf",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation\n",
    "5-fold cross-validation to assess model stability and generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003512f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "if X_train is not None:\n",
    "    print(\"=== 5-Fold Cross-Validation Results ===\\n\")\n",
    "    \n",
    "    cv_results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Use full training set for CV\n",
    "        if 'Logistic' in name:\n",
    "            X_cv = X_train_scaled\n",
    "        else:\n",
    "            X_cv = X_train\n",
    "        \n",
    "        # Perform CV\n",
    "        scores = cross_val_score(model, X_cv, y_train, cv=5, scoring='accuracy')\n",
    "        \n",
    "        cv_results.append({\n",
    "            'Model': name,\n",
    "            'Mean_CV_Score': scores.mean(),\n",
    "            'Std_CV_Score': scores.std(),\n",
    "            'Min_Score': scores.min(),\n",
    "            'Max_Score': scores.max()\n",
    "        })\n",
    "        \n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Mean Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "        print(f\"  Scores: {[f'{s:.4f}' for s in scores]}\")\n",
    "        print()\n",
    "    \n",
    "    # Visualize CV results\n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = np.arange(len(cv_df))\n",
    "    ax.bar(x, cv_df['Mean_CV_Score'], yerr=cv_df['Std_CV_Score'], \n",
    "           capsize=5, alpha=0.7, color=['blue', 'green', 'orange'])\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Cross-Validation Accuracy')\n",
    "    ax.set_title('5-Fold Cross-Validation Results')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(cv_df['Model'], rotation=15, ha='right')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Cross-validation complete\")\n",
    "else:\n",
    "    print(\"Cannot perform CV - data not prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1774ce4",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "Analyze model performance vs training set size to detect overfitting/underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d09467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curves\n",
    "if X_train is not None:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for idx, (name, model) in enumerate(models.items()):\n",
    "        # Prepare data\n",
    "        if 'Logistic' in name:\n",
    "            X_learn = X_train_scaled\n",
    "        else:\n",
    "            X_learn = X_train\n",
    "        \n",
    "        # Calculate learning curve\n",
    "        train_sizes, train_scores, val_scores = learning_curve(\n",
    "            model, X_learn, y_train,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Calculate means and stds\n",
    "        train_mean = train_scores.mean(axis=1)\n",
    "        train_std = train_scores.std(axis=1)\n",
    "        val_mean = val_scores.mean(axis=1)\n",
    "        val_std = val_scores.std(axis=1)\n",
    "        \n",
    "        # Plot\n",
    "        axes[idx].plot(train_sizes, train_mean, label='Training score', color='blue', marker='o')\n",
    "        axes[idx].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n",
    "                               alpha=0.15, color='blue')\n",
    "        \n",
    "        axes[idx].plot(train_sizes, val_mean, label='Validation score', color='red', marker='s')\n",
    "        axes[idx].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, \n",
    "                               alpha=0.15, color='red')\n",
    "        \n",
    "        axes[idx].set_xlabel('Training Set Size')\n",
    "        axes[idx].set_ylabel('Accuracy')\n",
    "        axes[idx].set_title(f'{name}\\nLearning Curve')\n",
    "        axes[idx].legend(loc='best')\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "        \n",
    "        # Analysis\n",
    "        gap = train_mean[-1] - val_mean[-1]\n",
    "        if gap > 0.1:\n",
    "            print(f\"{name}: Shows overfitting (train-val gap = {gap:.3f})\")\n",
    "        elif gap < 0.05:\n",
    "            print(f\"{name}: Well-generalized\")\n",
    "        else:\n",
    "            print(f\"{name}: Slight overfitting (gap = {gap:.3f})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nLearning curves analysis complete\")\n",
    "else:\n",
    "    print(\"Cannot generate - data not prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302e3e5",
   "metadata": {},
   "source": [
    "## Evaluation Summary\n",
    "Comprehensive comparison of all evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17381df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final evaluation summary\n",
    "if X_train is not None and predictions:\n",
    "    print(\"=== Complete Model Evaluation Summary ===\\n\")\n",
    "    \n",
    "    summary = []\n",
    "    for name, y_pred in predictions.items():\n",
    "        summary.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'F1-Score': f1_score(y_test, y_pred)\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n=== Model Selection Recommendation ===\")\n",
    "    best_f1 = summary_df.loc[summary_df['F1-Score'].idxmax(), 'Model']\n",
    "    best_precision = summary_df.loc[summary_df['Precision'].idxmax(), 'Model']\n",
    "    best_recall = summary_df.loc[summary_df['Recall'].idxmax(), 'Model']\n",
    "    \n",
    "    print(f\"Best F1-Score: {best_f1}\")\n",
    "    print(f\"Best Precision: {best_precision}\")\n",
    "    print(f\"Best Recall: {best_recall}\")\n",
    "    \n",
    "    print(\"\\nFor loan default prediction:\")\n",
    "    print(\"- High Recall is important to catch all potential defaults\")\n",
    "    print(\"- High Precision reduces false alarms\")\n",
    "    print(\"- F1-Score balances both concerns\")\n",
    "    \n",
    "    print(f\"\\nRecommended model: {best_f1} (best F1-score)\")\n",
    "else:\n",
    "    print(\"Cannot generate summary - evaluation incomplete\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
