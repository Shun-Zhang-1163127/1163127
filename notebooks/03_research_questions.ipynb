{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Questions Development\n",
    "\n",
    "**COMP647 Assignment 02 - Student ID: 1163127**\n",
    "\n",
    "This notebook develops research questions based on the exploratory data analysis (EDA) findings from the Lending Club loan dataset. Each research question is supported by evidence from our statistical analysis and correlation studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load EDA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# System utilities\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(\"Ready to develop research questions based on EDA findings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analysis_data(sample_size='10000'):\n",
    "    \"\"\"\n",
    "    Load the preprocessed dataset for research question analysis.\n",
    "    \n",
    "    This function loads the same dataset used in EDA to ensure consistency\n",
    "    in research question development and validation.\n",
    "    \n",
    "    Parameters:\n",
    "    sample_size (str): Size of sample to load\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Preprocessed lending data\n",
    "    \"\"\"\n",
    "    print(f\"Loading analysis dataset (sample size: {sample_size})...\")\n",
    "    \n",
    "    data_path = '../data/processed/'\n",
    "    accepted_file = f'accepted_sample_{sample_size}.csv'\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(data_path, accepted_file))\n",
    "        \n",
    "        # Basic preprocessing consistent with EDA\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        print(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "        print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Question Framework\n",
    "\n",
    "Based on EDA findings, we develop research questions that explore meaningful patterns in lending data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_key_variables(df):\n",
    "    \"\"\"\n",
    "    Identify key variables and relationships for research question development.\n",
    "    \n",
    "    This function analyzes the dataset to identify variables with strong\n",
    "    analytical potential based on data quality, variability, and business relevance.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataset\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis results for research question development\n",
    "    \"\"\"\n",
    "    print(\"Analyzing key variables for research question development...\")\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"No data available for analysis\")\n",
    "        return {}\n",
    "    \n",
    "    analysis_results = {\n",
    "        'dataset_overview': {},\n",
    "        'key_numeric_variables': [],\n",
    "        'key_categorical_variables': [],\n",
    "        'potential_target_variables': [],\n",
    "        'high_correlation_pairs': []\n",
    "    }\n",
    "    \n",
    "    # Dataset overview\n",
    "    analysis_results['dataset_overview'] = {\n",
    "        'total_loans': len(df),\n",
    "        'total_features': len(df.columns),\n",
    "        'data_completeness': ((df.count().sum()) / (len(df) * len(df.columns))) * 100\n",
    "    }\n",
    "    \n",
    "    # Identify high-quality numeric variables\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        missing_pct = (df[col].isnull().sum() / len(df)) * 100\n",
    "        unique_pct = (df[col].nunique() / len(df)) * 100\n",
    "        \n",
    "        # Select variables with good data quality and variability\n",
    "        if missing_pct < 15 and unique_pct > 1 and unique_pct < 95:\n",
    "            var_info = {\n",
    "                'variable': col,\n",
    "                'missing_pct': missing_pct,\n",
    "                'unique_values': df[col].nunique(),\n",
    "                'data_type': 'numeric',\n",
    "                'business_relevance': get_business_relevance(col)\n",
    "            }\n",
    "            analysis_results['key_numeric_variables'].append(var_info)\n",
    "    \n",
    "    # Identify categorical variables\n",
    "    categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        missing_pct = (df[col].isnull().sum() / len(df)) * 100\n",
    "        unique_count = df[col].nunique()\n",
    "        \n",
    "        # Select categorical variables with moderate cardinality\n",
    "        if missing_pct < 20 and 2 <= unique_count <= 20:\n",
    "            var_info = {\n",
    "                'variable': col,\n",
    "                'missing_pct': missing_pct,\n",
    "                'unique_values': unique_count,\n",
    "                'data_type': 'categorical',\n",
    "                'top_category': df[col].mode().iloc[0] if len(df[col].mode()) > 0 else None,\n",
    "                'business_relevance': get_business_relevance(col)\n",
    "            }\n",
    "            analysis_results['key_categorical_variables'].append(var_info)\n",
    "    \n",
    "    # Identify potential target variables based on business logic\n",
    "    target_candidates = []\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if any(keyword in col_lower for keyword in ['status', 'grade', 'outcome', 'result', 'default', 'paid']):\n",
    "            target_candidates.append({\n",
    "                'variable': col,\n",
    "                'rationale': f'Contains target-related keyword: {get_target_keyword(col)}',\n",
    "                'unique_values': df[col].nunique() if col in df.columns else 0\n",
    "            })\n",
    "    \n",
    "    analysis_results['potential_target_variables'] = target_candidates[:5]\n",
    "    \n",
    "    # Quick correlation analysis for top numeric variables\n",
    "    if len(analysis_results['key_numeric_variables']) >= 2:\n",
    "        top_numeric = [var['variable'] for var in analysis_results['key_numeric_variables'][:10]]\n",
    "        corr_matrix = df[top_numeric].corr()\n",
    "        \n",
    "        # Find strong correlations\n",
    "        strong_correlations = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                var1 = corr_matrix.columns[i]\n",
    "                var2 = corr_matrix.columns[j]\n",
    "                corr_value = corr_matrix.iloc[i, j]\n",
    "                \n",
    "                if not pd.isna(corr_value) and abs(corr_value) >= 0.4:\n",
    "                    strong_correlations.append({\n",
    "                        'var1': var1,\n",
    "                        'var2': var2,\n",
    "                        'correlation': corr_value,\n",
    "                        'strength': 'Strong' if abs(corr_value) >= 0.6 else 'Moderate'\n",
    "                    })\n",
    "        \n",
    "        analysis_results['high_correlation_pairs'] = sorted(\n",
    "            strong_correlations, \n",
    "            key=lambda x: abs(x['correlation']), \n",
    "            reverse=True\n",
    "        )[:8]\n",
    "    \n",
    "    # Display analysis summary\n",
    "    print(f\"\\nVariable Analysis Summary:\")\n",
    "    print(f\"  High-quality numeric variables: {len(analysis_results['key_numeric_variables'])}\")\n",
    "    print(f\"  Suitable categorical variables: {len(analysis_results['key_categorical_variables'])}\")\n",
    "    print(f\"  Potential target variables: {len(analysis_results['potential_target_variables'])}\")\n",
    "    print(f\"  Strong correlations found: {len(analysis_results['high_correlation_pairs'])}\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "def get_business_relevance(column_name):\n",
    "    \"\"\"\n",
    "    Determine business relevance of a variable based on its name.\n",
    "    \n",
    "    Parameters:\n",
    "    column_name (str): Name of the variable\n",
    "    \n",
    "    Returns:\n",
    "    str: Business relevance category\n",
    "    \"\"\"\n",
    "    col_lower = column_name.lower()\n",
    "    \n",
    "    if any(word in col_lower for word in ['amount', 'income', 'salary', 'balance']):\n",
    "        return 'Financial'\n",
    "    elif any(word in col_lower for word in ['rate', 'interest', 'apr', 'percent']):\n",
    "        return 'Risk/Pricing'\n",
    "    elif any(word in col_lower for word in ['term', 'time', 'month', 'year', 'duration']):\n",
    "        return 'Temporal'\n",
    "    elif any(word in col_lower for word in ['grade', 'score', 'rating', 'status']):\n",
    "        return 'Assessment'\n",
    "    elif any(word in col_lower for word in ['purpose', 'type', 'category', 'reason']):\n",
    "        return 'Categorical'\n",
    "    else:\n",
    "        return 'General'\n",
    "\n",
    "def get_target_keyword(column_name):\n",
    "    \"\"\"\n",
    "    Identify target-related keywords in column name.\n",
    "    \n",
    "    Parameters:\n",
    "    column_name (str): Name of the variable\n",
    "    \n",
    "    Returns:\n",
    "    str: Identified keyword\n",
    "    \"\"\"\n",
    "    col_lower = column_name.lower()\n",
    "    target_keywords = ['status', 'grade', 'outcome', 'result', 'default', 'paid']\n",
    "    \n",
    "    for keyword in target_keywords:\n",
    "        if keyword in col_lower:\n",
    "            return keyword\n",
    "    return 'target-related'\n",
    "\n",
    "print(\"Research question framework functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data and Perform Key Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for research question development\n",
    "print(\"=== LOADING DATA FOR RESEARCH QUESTION ANALYSIS ===\")\n",
    "df_loans = load_analysis_data(sample_size='10000')\n",
    "\n",
    "if df_loans is not None:\n",
    "    print(f\"\\nDataset ready for analysis: {df_loans.shape}\")\n",
    "    print(f\"Sample of available columns:\")\n",
    "    for i, col in enumerate(df_loans.columns[:15]):\n",
    "        print(f\"  {i+1:2d}. {col}\")\n",
    "    if len(df_loans.columns) > 15:\n",
    "        print(f\"  ... and {len(df_loans.columns) - 15} more columns\")\n",
    "else:\n",
    "    print(\"Failed to load dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform key variable analysis\n",
    "if df_loans is not None:\n",
    "    print(\"=== KEY VARIABLE ANALYSIS ===\")\n",
    "    key_variables = analyze_key_variables(df_loans)\n",
    "    \n",
    "    # Display key findings\n",
    "    if key_variables:\n",
    "        print(f\"\\n=== DATASET OVERVIEW ===\")\n",
    "        overview = key_variables['dataset_overview']\n",
    "        print(f\"Total loans: {overview.get('total_loans', 0):,}\")\n",
    "        print(f\"Total features: {overview.get('total_features', 0)}\")\n",
    "        print(f\"Data completeness: {overview.get('data_completeness', 0):.1f}%\")\n",
    "        \n",
    "        print(f\"\\n=== TOP NUMERIC VARIABLES FOR RESEARCH ===\")\n",
    "        numeric_vars = key_variables.get('key_numeric_variables', [])\n",
    "        for i, var in enumerate(numeric_vars[:8], 1):\n",
    "            print(f\"{i:2d}. {var['variable']:25} | {var['business_relevance']:12} | {var['missing_pct']:4.1f}% missing\")\n",
    "        \n",
    "        print(f\"\\n=== TOP CATEGORICAL VARIABLES FOR RESEARCH ===\")\n",
    "        cat_vars = key_variables.get('key_categorical_variables', [])\n",
    "        for i, var in enumerate(cat_vars[:6], 1):\n",
    "            print(f\"{i:2d}. {var['variable']:25} | {var['unique_values']:2d} categories | {var['business_relevance']:12}\")\n",
    "        \n",
    "        print(f\"\\n=== POTENTIAL TARGET VARIABLES ===\")\n",
    "        target_vars = key_variables.get('potential_target_variables', [])\n",
    "        if target_vars:\n",
    "            for i, var in enumerate(target_vars, 1):\n",
    "                print(f\"{i:2d}. {var['variable']:25} | {var['rationale']}\")\n",
    "        else:\n",
    "            print(\"No obvious target variables identified - will use loan characteristics as analysis focus\")\n",
    "        \n",
    "        print(f\"\\n=== STRONG CORRELATIONS FOR RESEARCH ===\")\n",
    "        correlations = key_variables.get('high_correlation_pairs', [])\n",
    "        if correlations:\n",
    "            for i, corr in enumerate(correlations[:5], 1):\n",
    "                print(f\"{i:2d}. {corr['var1']} vs {corr['var2']} | r = {corr['correlation']:.3f} ({corr['strength']})\")\n",
    "        else:\n",
    "            print(\"No strong correlations found with current threshold\")\n",
    "        \n",
    "        # Store results for research question development\n",
    "        research_foundation = key_variables\n",
    "    else:\n",
    "        print(\"Variable analysis failed\")\n",
    "        research_foundation = {}\n",
    "else:\n",
    "    print(\"Cannot perform variable analysis without data\")\n",
    "    research_foundation = {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}