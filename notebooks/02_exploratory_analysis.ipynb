{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "**COMP647 Assignment 02 - Student ID: 1163127**\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis of the Lending Club loan dataset to understand data patterns, relationships, and characteristics that will inform research question development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, pearsonr, spearmanr\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# System utilities\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration for better visualization\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Figure size configuration\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(sample_size='10000'):\n",
    "    \"\"\"\n",
    "    Load and preprocess sample datasets for exploratory data analysis.\n",
    "    \n",
    "    This function loads the sample datasets and applies the preprocessing\n",
    "    pipeline developed in the previous notebook to ensure data quality\n",
    "    for analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    sample_size (str): Size of sample to load ('1000', '10000', '50000')\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (preprocessed_accepted_df, raw_rejected_df)\n",
    "    \"\"\"\n",
    "    print(f\"Loading sample datasets for EDA (size: {sample_size})...\")\n",
    "    \n",
    "    # Define file paths\n",
    "    data_path = '../data/processed/'\n",
    "    accepted_file = f'accepted_sample_{sample_size}.csv'\n",
    "    rejected_file = f'rejected_sample_{sample_size}.csv'\n",
    "    \n",
    "    try:\n",
    "        # Load datasets\n",
    "        df_accepted = pd.read_csv(os.path.join(data_path, accepted_file))\n",
    "        df_rejected = pd.read_csv(os.path.join(data_path, rejected_file))\n",
    "        \n",
    "        print(f\"Raw accepted loans: {df_accepted.shape[0]:,} rows, {df_accepted.shape[1]} columns\")\n",
    "        print(f\"Raw rejected loans: {df_rejected.shape[0]:,} rows, {df_rejected.shape[1]} columns\")\n",
    "        \n",
    "        # For EDA, we'll work with the accepted loans dataset\n",
    "        # Apply basic preprocessing for better analysis\n",
    "        df_processed = df_accepted.copy()\n",
    "        \n",
    "        # Remove duplicate rows\n",
    "        initial_rows = len(df_processed)\n",
    "        df_processed = df_processed.drop_duplicates()\n",
    "        duplicates_removed = initial_rows - len(df_processed)\n",
    "        \n",
    "        if duplicates_removed > 0:\n",
    "            print(f\"Removed {duplicates_removed:,} duplicate rows\")\n",
    "        \n",
    "        print(f\"Preprocessed accepted loans ready for EDA: {df_processed.shape}\")\n",
    "        print(\"Data loading completed successfully!\")\n",
    "        \n",
    "        return df_processed, df_rejected\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading files: {e}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Overview and Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_structure(df):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of dataset structure and characteristics.\n",
    "    \n",
    "    This function provides detailed insights into the dataset structure,\n",
    "    including data types, missing values, unique values, and basic \n",
    "    statistical properties to guide the EDA process.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to analyze\n",
    "    \n",
    "    Returns:\n",
    "    dict: Comprehensive dataset analysis results\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing dataset structure for {df.shape[0]:,} rows and {df.shape[1]} columns\")\n",
    "    \n",
    "    # Basic dataset information\n",
    "    dataset_info = {\n",
    "        'shape': df.shape,\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'total_missing': df.isnull().sum().sum(),\n",
    "        'data_types': df.dtypes.value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    # Column analysis\n",
    "    column_analysis = []\n",
    "    for col in df.columns:\n",
    "        col_info = {\n",
    "            'column': col,\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'missing_count': df[col].isnull().sum(),\n",
    "            'missing_pct': (df[col].isnull().sum() / len(df)) * 100,\n",
    "            'unique_count': df[col].nunique(),\n",
    "            'unique_pct': (df[col].nunique() / len(df)) * 100\n",
    "        }\n",
    "        \n",
    "        # Add data type specific analysis\n",
    "        if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            col_info['min_value'] = df[col].min()\n",
    "            col_info['max_value'] = df[col].max()\n",
    "            col_info['mean_value'] = df[col].mean()\n",
    "            col_info['std_value'] = df[col].std()\n",
    "        else:\n",
    "            # For categorical data, get most common values\n",
    "            value_counts = df[col].value_counts()\n",
    "            col_info['most_common'] = value_counts.index[0] if len(value_counts) > 0 else None\n",
    "            col_info['most_common_count'] = value_counts.iloc[0] if len(value_counts) > 0 else 0\n",
    "        \n",
    "        column_analysis.append(col_info)\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    columns_df = pd.DataFrame(column_analysis)\n",
    "    \n",
    "    # Data quality categorization\n",
    "    quality_summary = {\n",
    "        'high_quality_cols': len(columns_df[columns_df['missing_pct'] < 5]),\n",
    "        'medium_quality_cols': len(columns_df[(columns_df['missing_pct'] >= 5) & (columns_df['missing_pct'] < 20)]),\n",
    "        'low_quality_cols': len(columns_df[columns_df['missing_pct'] >= 20]),\n",
    "        'unique_identifier_cols': len(columns_df[columns_df['unique_pct'] > 95]),\n",
    "        'categorical_cols': len(columns_df[(columns_df['unique_count'] < 20) & (~columns_df['dtype'].str.contains('int|float'))]),\n",
    "        'numeric_cols': len(columns_df[columns_df['dtype'].str.contains('int|float')])\n",
    "    }\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nDataset Structure Summary:\")\n",
    "    print(f\"  Shape: {dataset_info['shape']}\")\n",
    "    print(f\"  Memory usage: {dataset_info['memory_usage_mb']:.2f} MB\")\n",
    "    print(f\"  Total missing values: {dataset_info['total_missing']:,}\")\n",
    "    \n",
    "    print(f\"\\nData Types Distribution:\")\n",
    "    for dtype, count in dataset_info['data_types'].items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    print(f\"\\nData Quality Assessment:\")\n",
    "    print(f\"  High quality columns (<5% missing): {quality_summary['high_quality_cols']}\")\n",
    "    print(f\"  Medium quality columns (5-20% missing): {quality_summary['medium_quality_cols']}\")\n",
    "    print(f\"  Low quality columns (>20% missing): {quality_summary['low_quality_cols']}\")\n",
    "    print(f\"  Potential identifier columns (>95% unique): {quality_summary['unique_identifier_cols']}\")\n",
    "    print(f\"  Categorical columns: {quality_summary['categorical_cols']}\")\n",
    "    print(f\"  Numeric columns: {quality_summary['numeric_cols']}\")\n",
    "    \n",
    "    return {\n",
    "        'dataset_info': dataset_info,\n",
    "        'column_analysis': columns_df,\n",
    "        'quality_summary': quality_summary\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Summary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistical_summary(df):\n",
    "    \"\"\"\n",
    "    Generate comprehensive statistical summaries for numeric and categorical variables.\n",
    "    \n",
    "    This function creates detailed statistical summaries that go beyond basic\n",
    "    describe() to include skewness, kurtosis, and quartile analysis for numeric\n",
    "    variables, and frequency analysis for categorical variables.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to analyze\n",
    "    \n",
    "    Returns:\n",
    "    dict: Statistical summary results\n",
    "    \"\"\"\n",
    "    print(\"Generating comprehensive statistical summaries...\")\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    print(f\"Analyzing {len(numeric_cols)} numeric and {len(categorical_cols)} categorical columns\")\n",
    "    \n",
    "    # Numeric variables statistical summary\n",
    "    numeric_summary = {}\n",
    "    if len(numeric_cols) > 0:\n",
    "        # Enhanced numeric summary with additional statistics\n",
    "        numeric_stats = df[numeric_cols].describe()\n",
    "        \n",
    "        # Add skewness and kurtosis\n",
    "        for col in numeric_cols:\n",
    "            if df[col].notna().sum() > 0:  # Only if column has non-null values\n",
    "                numeric_summary[col] = {\n",
    "                    'count': df[col].count(),\n",
    "                    'mean': df[col].mean(),\n",
    "                    'median': df[col].median(),\n",
    "                    'std': df[col].std(),\n",
    "                    'min': df[col].min(),\n",
    "                    'max': df[col].max(),\n",
    "                    'q25': df[col].quantile(0.25),\n",
    "                    'q75': df[col].quantile(0.75),\n",
    "                    'skewness': df[col].skew(),\n",
    "                    'kurtosis': df[col].kurtosis(),\n",
    "                    'iqr': df[col].quantile(0.75) - df[col].quantile(0.25),\n",
    "                    'cv': df[col].std() / df[col].mean() if df[col].mean() != 0 else 0,\n",
    "                    'zeros': (df[col] == 0).sum(),\n",
    "                    'zeros_pct': ((df[col] == 0).sum() / len(df)) * 100\n",
    "                }\n",
    "    \n",
    "    # Categorical variables summary\n",
    "    categorical_summary = {}\n",
    "    if len(categorical_cols) > 0:\n",
    "        for col in categorical_cols[:10]:  # Limit to first 10 categorical columns\n",
    "            value_counts = df[col].value_counts()\n",
    "            categorical_summary[col] = {\n",
    "                'unique_count': df[col].nunique(),\n",
    "                'most_common': value_counts.index[0] if len(value_counts) > 0 else None,\n",
    "                'most_common_count': value_counts.iloc[0] if len(value_counts) > 0 else 0,\n",
    "                'most_common_pct': (value_counts.iloc[0] / len(df)) * 100 if len(value_counts) > 0 else 0,\n",
    "                'top_5_values': value_counts.head().to_dict(),\n",
    "                'missing_count': df[col].isnull().sum(),\n",
    "                'missing_pct': (df[col].isnull().sum() / len(df)) * 100\n",
    "            }\n",
    "    \n",
    "    # Display key insights\n",
    "    print(f\"\\nKey Numeric Variable Insights:\")\n",
    "    if numeric_summary:\n",
    "        # Find highly skewed variables\n",
    "        highly_skewed = [(col, stats['skewness']) for col, stats in numeric_summary.items() \n",
    "                        if abs(stats['skewness']) > 2]\n",
    "        if highly_skewed:\n",
    "            print(f\"  Highly skewed variables (|skewness| > 2): {len(highly_skewed)}\")\n",
    "            for col, skew in highly_skewed[:5]:\n",
    "                print(f\"    {col}: {skew:.2f}\")\n",
    "        \n",
    "        # Find high variability variables\n",
    "        high_cv = [(col, stats['cv']) for col, stats in numeric_summary.items() \n",
    "                  if stats['cv'] > 1]\n",
    "        if high_cv:\n",
    "            print(f\"  High variability variables (CV > 1): {len(high_cv)}\")\n",
    "    \n",
    "    print(f\"\\nKey Categorical Variable Insights:\")\n",
    "    if categorical_summary:\n",
    "        # Find highly concentrated categorical variables\n",
    "        concentrated = [(col, stats['most_common_pct']) for col, stats in categorical_summary.items() \n",
    "                       if stats['most_common_pct'] > 90]\n",
    "        if concentrated:\n",
    "            print(f\"  Highly concentrated variables (>90% in one category): {len(concentrated)}\")\n",
    "        \n",
    "        # Find high cardinality categorical variables\n",
    "        high_cardinality = [(col, stats['unique_count']) for col, stats in categorical_summary.items() \n",
    "                           if stats['unique_count'] > 50]\n",
    "        if high_cardinality:\n",
    "            print(f\"  High cardinality variables (>50 categories): {len(high_cardinality)}\")\n",
    "    \n",
    "    return {\n",
    "        'numeric_summary': numeric_summary,\n",
    "        'categorical_summary': categorical_summary,\n",
    "        'numeric_columns': list(numeric_cols),\n",
    "        'categorical_columns': list(categorical_cols)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization Framework\n",
    "\n",
    "Comprehensive visualization functions for distribution analysis and pattern discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distribution_plots(df, columns, plot_type='histogram'):\n",
    "    \"\"\"\n",
    "    Create distribution plots for numeric variables to understand data patterns.\n",
    "    \n",
    "    This function generates various types of distribution plots including histograms,\n",
    "    box plots, and violin plots to analyze the distribution characteristics of\n",
    "    numeric variables in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset containing the variables\n",
    "    columns (list): List of column names to plot\n",
    "    plot_type (str): Type of plot ('histogram', 'boxplot', 'violin')\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays plots\n",
    "    \"\"\"\n",
    "    if not columns or len(columns) == 0:\n",
    "        print(\"No columns provided for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Limit to first 6 columns to avoid overwhelming output\n",
    "    columns = columns[:6]\n",
    "    \n",
    "    print(f\"Creating {plot_type} plots for {len(columns)} variables...\")\n",
    "    \n",
    "    # Set up subplot grid\n",
    "    n_cols = min(3, len(columns))\n",
    "    n_rows = (len(columns) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if len(columns) == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        if col in df.columns and df[col].notna().sum() > 0:\n",
    "            # Remove outliers for better visualization\n",
    "            data = df[col].dropna()\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            filtered_data = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "            \n",
    "            if plot_type == 'histogram':\n",
    "                axes[i].hist(filtered_data, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "                axes[i].axvline(filtered_data.mean(), color='red', linestyle='--', label=f'Mean: {filtered_data.mean():.2f}')\n",
    "                axes[i].axvline(filtered_data.median(), color='green', linestyle='--', label=f'Median: {filtered_data.median():.2f}')\n",
    "                axes[i].legend()\n",
    "                \n",
    "            elif plot_type == 'boxplot':\n",
    "                axes[i].boxplot(filtered_data)\n",
    "                \n",
    "            elif plot_type == 'violin':\n",
    "                # Use seaborn for violin plots\n",
    "                sns.violinplot(y=filtered_data, ax=axes[i])\n",
    "            \n",
    "            axes[i].set_title(f'{col}\\n(n={len(filtered_data):,}, outliers removed)', fontsize=10)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Distribution plots completed for {len(columns)} variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Execution - Load Data and Begin Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data for exploratory analysis\n",
    "print(\"=== LOADING DATA FOR EXPLORATORY DATA ANALYSIS ===\")\n",
    "df_loans, df_rejected = load_preprocessed_data(sample_size='10000')\n",
    "\n",
    "if df_loans is not None:\n",
    "    print(f\"\\nSuccessfully loaded loans dataset: {df_loans.shape}\")\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(f\"\\n=== DATASET BASIC INFORMATION ===\")\n",
    "    print(f\"Shape: {df_loans.shape}\")\n",
    "    print(f\"Memory usage: {df_loans.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"Missing values: {df_loans.isnull().sum().sum():,}\")\n",
    "    \n",
    "    # Show sample of column names\n",
    "    print(f\"\\nSample columns (first 10):\")\n",
    "    for i, col in enumerate(df_loans.columns[:10]):\n",
    "        print(f\"  {i+1:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully - ready for EDA!\")\n",
    "else:\n",
    "    print(\"Failed to load data - please check file paths and ensure sample files exist\")"
   ]
  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3 (ipykernel)\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.9.16\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}