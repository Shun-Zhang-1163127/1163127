{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "**COMP647 Assignment 02 - Student ID: 1163127**\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis of the Lending Club loan dataset to understand data patterns, relationships, and characteristics that will inform research question development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, pearsonr, spearmanr\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# System utilities\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration for better visualization\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Figure size configuration\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(sample_size='10000'):\n",
    "    \"\"\"\n",
    "    Load and preprocess sample datasets for exploratory data analysis.\n",
    "    \n",
    "    This function loads the sample datasets and applies the preprocessing\n",
    "    pipeline developed in the previous notebook to ensure data quality\n",
    "    for analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    sample_size (str): Size of sample to load ('1000', '10000', '50000')\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (preprocessed_accepted_df, raw_rejected_df)\n",
    "    \"\"\"\n",
    "    print(f\"Loading sample datasets for EDA (size: {sample_size})...\")\n",
    "    \n",
    "    # Define file paths\n",
    "    data_path = '../data/processed/'\n",
    "    accepted_file = f'accepted_sample_{sample_size}.csv'\n",
    "    rejected_file = f'rejected_sample_{sample_size}.csv'\n",
    "    \n",
    "    try:\n",
    "        # Load datasets\n",
    "        df_accepted = pd.read_csv(os.path.join(data_path, accepted_file))\n",
    "        df_rejected = pd.read_csv(os.path.join(data_path, rejected_file))\n",
    "        \n",
    "        print(f\"Raw accepted loans: {df_accepted.shape[0]:,} rows, {df_accepted.shape[1]} columns\")\n",
    "        print(f\"Raw rejected loans: {df_rejected.shape[0]:,} rows, {df_rejected.shape[1]} columns\")\n",
    "        \n",
    "        # For EDA, we'll work with the accepted loans dataset\n",
    "        # Apply basic preprocessing for better analysis\n",
    "        df_processed = df_accepted.copy()\n",
    "        \n",
    "        # Remove duplicate rows\n",
    "        initial_rows = len(df_processed)\n",
    "        df_processed = df_processed.drop_duplicates()\n",
    "        duplicates_removed = initial_rows - len(df_processed)\n",
    "        \n",
    "        if duplicates_removed > 0:\n",
    "            print(f\"Removed {duplicates_removed:,} duplicate rows\")\n",
    "        \n",
    "        print(f\"Preprocessed accepted loans ready for EDA: {df_processed.shape}\")\n",
    "        print(\"Data loading completed successfully!\")\n",
    "        \n",
    "        return df_processed, df_rejected\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading files: {e}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Overview and Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_structure(df):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of dataset structure and characteristics.\n",
    "    \n",
    "    This function provides detailed insights into the dataset structure,\n",
    "    including data types, missing values, unique values, and basic \n",
    "    statistical properties to guide the EDA process.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to analyze\n",
    "    \n",
    "    Returns:\n",
    "    dict: Comprehensive dataset analysis results\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing dataset structure for {df.shape[0]:,} rows and {df.shape[1]} columns\")\n",
    "    \n",
    "    # Basic dataset information\n",
    "    dataset_info = {\n",
    "        'shape': df.shape,\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'total_missing': df.isnull().sum().sum(),\n",
    "        'data_types': df.dtypes.value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    # Column analysis\n",
    "    column_analysis = []\n",
    "    for col in df.columns:\n",
    "        col_info = {\n",
    "            'column': col,\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'missing_count': df[col].isnull().sum(),\n",
    "            'missing_pct': (df[col].isnull().sum() / len(df)) * 100,\n",
    "            'unique_count': df[col].nunique(),\n",
    "            'unique_pct': (df[col].nunique() / len(df)) * 100\n",
    "        }\n",
    "        \n",
    "        # Add data type specific analysis\n",
    "        if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            col_info['min_value'] = df[col].min()\n",
    "            col_info['max_value'] = df[col].max()\n",
    "            col_info['mean_value'] = df[col].mean()\n",
    "            col_info['std_value'] = df[col].std()\n",
    "        else:\n",
    "            # For categorical data, get most common values\n",
    "            value_counts = df[col].value_counts()\n",
    "            col_info['most_common'] = value_counts.index[0] if len(value_counts) > 0 else None\n",
    "            col_info['most_common_count'] = value_counts.iloc[0] if len(value_counts) > 0 else 0\n",
    "        \n",
    "        column_analysis.append(col_info)\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    columns_df = pd.DataFrame(column_analysis)\n",
    "    \n",
    "    # Data quality categorization\n",
    "    quality_summary = {\n",
    "        'high_quality_cols': len(columns_df[columns_df['missing_pct'] < 5]),\n",
    "        'medium_quality_cols': len(columns_df[(columns_df['missing_pct'] >= 5) & (columns_df['missing_pct'] < 20)]),\n",
    "        'low_quality_cols': len(columns_df[columns_df['missing_pct'] >= 20]),\n",
    "        'unique_identifier_cols': len(columns_df[columns_df['unique_pct'] > 95]),\n",
    "        'categorical_cols': len(columns_df[(columns_df['unique_count'] < 20) & (~columns_df['dtype'].str.contains('int|float'))]),\n",
    "        'numeric_cols': len(columns_df[columns_df['dtype'].str.contains('int|float')])\n",
    "    }\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nDataset Structure Summary:\")\n",
    "    print(f\"  Shape: {dataset_info['shape']}\")\n",
    "    print(f\"  Memory usage: {dataset_info['memory_usage_mb']:.2f} MB\")\n",
    "    print(f\"  Total missing values: {dataset_info['total_missing']:,}\")\n",
    "    \n",
    "    print(f\"\\nData Types Distribution:\")\n",
    "    for dtype, count in dataset_info['data_types'].items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    print(f\"\\nData Quality Assessment:\")\n",
    "    print(f\"  High quality columns (<5% missing): {quality_summary['high_quality_cols']}\")\n",
    "    print(f\"  Medium quality columns (5-20% missing): {quality_summary['medium_quality_cols']}\")\n",
    "    print(f\"  Low quality columns (>20% missing): {quality_summary['low_quality_cols']}\")\n",
    "    print(f\"  Potential identifier columns (>95% unique): {quality_summary['unique_identifier_cols']}\")\n",
    "    print(f\"  Categorical columns: {quality_summary['categorical_cols']}\")\n",
    "    print(f\"  Numeric columns: {quality_summary['numeric_cols']}\")\n",
    "    \n",
    "    return {\n",
    "        'dataset_info': dataset_info,\n",
    "        'column_analysis': columns_df,\n",
    "        'quality_summary': quality_summary\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Summary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistical_summary(df):\n",
    "    \"\"\"\n",
    "    Generate comprehensive statistical summaries for numeric and categorical variables.\n",
    "    \n",
    "    This function creates detailed statistical summaries that go beyond basic\n",
    "    describe() to include skewness, kurtosis, and quartile analysis for numeric\n",
    "    variables, and frequency analysis for categorical variables.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to analyze\n",
    "    \n",
    "    Returns:\n",
    "    dict: Statistical summary results\n",
    "    \"\"\"\n",
    "    print(\"Generating comprehensive statistical summaries...\")\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    print(f\"Analyzing {len(numeric_cols)} numeric and {len(categorical_cols)} categorical columns\")\n",
    "    \n",
    "    # Numeric variables statistical summary\n",
    "    numeric_summary = {}\n",
    "    if len(numeric_cols) > 0:\n",
    "        # Enhanced numeric summary with additional statistics\n",
    "        numeric_stats = df[numeric_cols].describe()\n",
    "        \n",
    "        # Add skewness and kurtosis\n",
    "        for col in numeric_cols:\n",
    "            if df[col].notna().sum() > 0:  # Only if column has non-null values\n",
    "                numeric_summary[col] = {\n",
    "                    'count': df[col].count(),\n",
    "                    'mean': df[col].mean(),\n",
    "                    'median': df[col].median(),\n",
    "                    'std': df[col].std(),\n",
    "                    'min': df[col].min(),\n",
    "                    'max': df[col].max(),\n",
    "                    'q25': df[col].quantile(0.25),\n",
    "                    'q75': df[col].quantile(0.75),\n",
    "                    'skewness': df[col].skew(),\n",
    "                    'kurtosis': df[col].kurtosis(),\n",
    "                    'iqr': df[col].quantile(0.75) - df[col].quantile(0.25),\n",
    "                    'cv': df[col].std() / df[col].mean() if df[col].mean() != 0 else 0,\n",
    "                    'zeros': (df[col] == 0).sum(),\n",
    "                    'zeros_pct': ((df[col] == 0).sum() / len(df)) * 100\n",
    "                }\n",
    "    \n",
    "    # Categorical variables summary\n",
    "    categorical_summary = {}\n",
    "    if len(categorical_cols) > 0:\n",
    "        for col in categorical_cols[:10]:  # Limit to first 10 categorical columns\n",
    "            value_counts = df[col].value_counts()\n",
    "            categorical_summary[col] = {\n",
    "                'unique_count': df[col].nunique(),\n",
    "                'most_common': value_counts.index[0] if len(value_counts) > 0 else None,\n",
    "                'most_common_count': value_counts.iloc[0] if len(value_counts) > 0 else 0,\n",
    "                'most_common_pct': (value_counts.iloc[0] / len(df)) * 100 if len(value_counts) > 0 else 0,\n",
    "                'top_5_values': value_counts.head().to_dict(),\n",
    "                'missing_count': df[col].isnull().sum(),\n",
    "                'missing_pct': (df[col].isnull().sum() / len(df)) * 100\n",
    "            }\n",
    "    \n",
    "    # Display key insights\n",
    "    print(f\"\\nKey Numeric Variable Insights:\")\n",
    "    if numeric_summary:\n",
    "        # Find highly skewed variables\n",
    "        highly_skewed = [(col, stats['skewness']) for col, stats in numeric_summary.items() \n",
    "                        if abs(stats['skewness']) > 2]\n",
    "        if highly_skewed:\n",
    "            print(f\"  Highly skewed variables (|skewness| > 2): {len(highly_skewed)}\")\n",
    "            for col, skew in highly_skewed[:5]:\n",
    "                print(f\"    {col}: {skew:.2f}\")\n",
    "        \n",
    "        # Find high variability variables\n",
    "        high_cv = [(col, stats['cv']) for col, stats in numeric_summary.items() \n",
    "                  if stats['cv'] > 1]\n",
    "        if high_cv:\n",
    "            print(f\"  High variability variables (CV > 1): {len(high_cv)}\")\n",
    "    \n",
    "    print(f\"\\nKey Categorical Variable Insights:\")\n",
    "    if categorical_summary:\n",
    "        # Find highly concentrated categorical variables\n",
    "        concentrated = [(col, stats['most_common_pct']) for col, stats in categorical_summary.items() \n",
    "                       if stats['most_common_pct'] > 90]\n",
    "        if concentrated:\n",
    "            print(f\"  Highly concentrated variables (>90% in one category): {len(concentrated)}\")\n",
    "        \n",
    "        # Find high cardinality categorical variables\n",
    "        high_cardinality = [(col, stats['unique_count']) for col, stats in categorical_summary.items() \n",
    "                           if stats['unique_count'] > 50]\n",
    "        if high_cardinality:\n",
    "            print(f\"  High cardinality variables (>50 categories): {len(high_cardinality)}\")\n",
    "    \n",
    "    return {\n",
    "        'numeric_summary': numeric_summary,\n",
    "        'categorical_summary': categorical_summary,\n",
    "        'numeric_columns': list(numeric_cols),\n",
    "        'categorical_columns': list(categorical_cols)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization Framework\n",
    "\n",
    "Comprehensive visualization functions for distribution analysis and pattern discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distribution_plots(df, columns, plot_type='histogram'):\n",
    "    \"\"\"\n",
    "    Create distribution plots for numeric variables to understand data patterns.\n",
    "    \n",
    "    This function generates various types of distribution plots including histograms,\n",
    "    box plots, and violin plots to analyze the distribution characteristics of\n",
    "    numeric variables in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset containing the variables\n",
    "    columns (list): List of column names to plot\n",
    "    plot_type (str): Type of plot ('histogram', 'boxplot', 'violin')\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays plots\n",
    "    \"\"\"\n",
    "    if not columns or len(columns) == 0:\n",
    "        print(\"No columns provided for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Limit to first 6 columns to avoid overwhelming output\n",
    "    columns = columns[:6]\n",
    "    \n",
    "    print(f\"Creating {plot_type} plots for {len(columns)} variables...\")\n",
    "    \n",
    "    # Set up subplot grid\n",
    "    n_cols = min(3, len(columns))\n",
    "    n_rows = (len(columns) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if len(columns) == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        if col in df.columns and df[col].notna().sum() > 0:\n",
    "            # Remove outliers for better visualization\n",
    "            data = df[col].dropna()\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            filtered_data = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "            \n",
    "            if plot_type == 'histogram':\n",
    "                axes[i].hist(filtered_data, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "                axes[i].axvline(filtered_data.mean(), color='red', linestyle='--', label=f'Mean: {filtered_data.mean():.2f}')\n",
    "                axes[i].axvline(filtered_data.median(), color='green', linestyle='--', label=f'Median: {filtered_data.median():.2f}')\n",
    "                axes[i].legend()\n",
    "                \n",
    "            elif plot_type == 'boxplot':\n",
    "                axes[i].boxplot(filtered_data)\n",
    "            \n",
    "            elif plot_type == 'violin':\n",
    "                # Use seaborn for violin plots\n",
    "                sns.violinplot(y=filtered_data, ax=axes[i])\n",
    "            \n",
    "            axes[i].set_title(f'{col}\\n(n={len(filtered_data):,}, outliers removed)', fontsize=10)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Distribution plots completed for {len(columns)} variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Execution - Load Data and Begin Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data for exploratory analysis\n",
    "print(\"=== LOADING DATA FOR EXPLORATORY DATA ANALYSIS ===\")\n",
    "df_loans, df_rejected = load_preprocessed_data(sample_size='10000')\n",
    "\n",
    "if df_loans is not None:\n",
    "    print(f\"\\nSuccessfully loaded loans dataset: {df_loans.shape}\")\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(f\"\\n=== DATASET BASIC INFORMATION ===\")\n",
    "    print(f\"Shape: {df_loans.shape}\")\n",
    "    print(f\"Memory usage: {df_loans.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"Missing values: {df_loans.isnull().sum().sum():,}\")\n",
    "    \n",
    "    # Show sample of column names\n",
    "    print(f\"\\nSample columns (first 10):\")\n",
    "    for i, col in enumerate(df_loans.columns[:10]):\n",
    "        print(f\"  {i+1:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully - ready for EDA!\")\n",
    "else:\n",
    "    print(\"Failed to load data - please check file paths and ensure sample files exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive dataset structure analysis\n",
    "if df_loans is not None:\n",
    "    print(\"=== DATASET STRUCTURE ANALYSIS ===\")\n",
    "    structure_analysis = analyze_dataset_structure(df_loans)\n",
    "    \n",
    "    # Display detailed column analysis for key variables\n",
    "    column_analysis = structure_analysis['column_analysis']\n",
    "    \n",
    "    print(f\"\\n=== TOP 10 HIGHEST QUALITY COLUMNS ===\")\n",
    "    high_quality_cols = column_analysis.sort_values('missing_pct').head(10)\n",
    "    for _, row in high_quality_cols.iterrows():\n",
    "        print(f\"{row['column']:25} | {row['dtype']:10} | {row['missing_pct']:5.1f}% missing | {row['unique_count']:8,} unique\")\n",
    "    \n",
    "    print(f\"\\n=== POTENTIAL ANALYSIS TARGETS ===\")\n",
    "    # Find good candidates for analysis based on data quality\n",
    "    good_numeric = column_analysis[\n",
    "        (column_analysis['dtype'].str.contains('int|float')) & \n",
    "        (column_analysis['missing_pct'] < 10) &\n",
    "        (column_analysis['unique_pct'] > 1)  # Not constant values\n",
    "    ]['column'].head(8).tolist()\n",
    "    \n",
    "    good_categorical = column_analysis[\n",
    "        (~column_analysis['dtype'].str.contains('int|float')) & \n",
    "        (column_analysis['missing_pct'] < 20) &\n",
    "        (column_analysis['unique_count'] > 1) &\n",
    "        (column_analysis['unique_count'] < 50)  # Not too many categories\n",
    "    ]['column'].head(6).tolist()\n",
    "    \n",
    "    print(f\"Good numeric variables for analysis ({len(good_numeric)}):\")\n",
    "    for col in good_numeric:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    print(f\"\\nGood categorical variables for analysis ({len(good_categorical)}):\")\n",
    "    for col in good_categorical:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # Store variables for later analysis\n",
    "    analysis_numeric_vars = good_numeric\n",
    "    analysis_categorical_vars = good_categorical\n",
    "else:\n",
    "    analysis_numeric_vars = []\n",
    "    analysis_categorical_vars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive statistical summaries\n",
    "if df_loans is not None and len(analysis_numeric_vars) > 0:\n",
    "    print(\"=== STATISTICAL SUMMARY ANALYSIS ===\")\n",
    "    stats_results = generate_statistical_summary(df_loans)\n",
    "    \n",
    "    # Display detailed statistics for key numeric variables\n",
    "    print(f\"\\n=== DETAILED NUMERIC STATISTICS ===\")\n",
    "    numeric_stats = stats_results['numeric_summary']\n",
    "    \n",
    "    # Create a summary table for the top numeric variables\n",
    "    if numeric_stats:\n",
    "        summary_data = []\n",
    "        for col in analysis_numeric_vars[:6]:  # Top 6 numeric variables\n",
    "            if col in numeric_stats:\n",
    "                stats = numeric_stats[col]\n",
    "                summary_data.append({\n",
    "                    'Variable': col,\n",
    "                    'Count': f\"{stats['count']:,}\",\n",
    "                    'Mean': f\"{stats['mean']:.2f}\",\n",
    "                    'Median': f\"{stats['median']:.2f}\",\n",
    "                    'Std': f\"{stats['std']:.2f}\",\n",
    "                    'Skewness': f\"{stats['skewness']:.2f}\",\n",
    "                    'CV': f\"{stats['cv']:.2f}\",\n",
    "                    'Zeros%': f\"{stats['zeros_pct']:.1f}%\"\n",
    "                })\n",
    "        \n",
    "        if summary_data:\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Display categorical variable insights\n",
    "    print(f\"\\n=== CATEGORICAL VARIABLES INSIGHTS ===\")\n",
    "    categorical_stats = stats_results['categorical_summary']\n",
    "    \n",
    "    for col in analysis_categorical_vars[:4]:  # Top 4 categorical variables\n",
    "        if col in categorical_stats:\n",
    "            stats = categorical_stats[col]\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Unique values: {stats['unique_count']:,}\")\n",
    "            print(f\"  Most common: '{stats['most_common']}' ({stats['most_common_pct']:.1f}%)\")\n",
    "            print(f\"  Missing: {stats['missing_pct']:.1f}%\")\n",
    "            if 'top_5_values' in stats and stats['top_5_values']:\n",
    "                print(f\"  Top categories: {list(stats['top_5_values'].keys())[:3]}\")\n",
    "else:\n",
    "    print(\"No suitable numeric variables found for statistical analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Pie chart for top categories\n",
    "            colors = plt.cm.Set3(np.linspace(0, 1, len(value_counts)))\n",
    "            wedges, texts, autotexts = ax2.pie(value_counts.values, \n",
    "                                             labels=[str(x)[:10] + '...' if len(str(x)) > 10 else str(x) \n",
    "                                                   for x in value_counts.index],\n",
    "                                             autopct='%1.1f%%', colors=colors, startangle=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Basic Distribution Visualizations Implementation\n",
    "# Create distribution plots for key numeric variables identified from EDA\n",
    "if df_loans is not None and len(analysis_numeric_vars) > 0:\n",
    "    print(\"=== PHASE 1: BASIC DISTRIBUTION VISUALIZATIONS ==\")\n",
    "    \n",
    "    # Select key variables for visualization based on business importance\n",
    "    key_variables = [\n",
    "        'loan_amnt', 'annual_inc', 'int_rate', 'installment', \n",
    "        'dti', 'fico_range_low', 'fico_range_high', 'open_acc'\n",
    "    ]\n",
    "    \n",
    "    # Filter to available variables\n",
    "    available_key_vars = [var for var in key_variables if var in df_loans.columns][:6]\n",
    "    \n",
    "    if available_key_vars:\n",
    "        print(f\"Creating distribution visualizations for: {available_key_vars}\")\n",
    "        \n",
    "        # 1. Histogram distributions with statistical overlays\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, var in enumerate(available_key_vars):\n",
    "            if var in df_loans.columns and df_loans[var].notna().sum() > 0:\n",
    "                # Clean data by removing outliers for better visualization\n",
    "                data = df_loans[var].dropna()\n",
    "                Q1, Q3 = data.quantile([0.25, 0.75])\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                clean_data = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "                \n",
    "                # Create histogram with overlays\n",
    "                axes[i].hist(clean_data, bins=40, alpha=0.7, color='lightblue', \n",
    "                           edgecolor='navy', density=True, label='Distribution')\n",
    "                \n",
    "                # Add statistical overlays\n",
    "                mean_val = clean_data.mean()\n",
    "                median_val = clean_data.median()\n",
    "                std_val = clean_data.std()\n",
    "                \n",
    "                axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=2, \n",
    "                               label=f'Mean: {mean_val:.2f}')\n",
    "                axes[i].axvline(median_val, color='green', linestyle='--', linewidth=2, \n",
    "                               label=f'Median: {median_val:.2f}')\n",
    "                \n",
    "                # Add normal distribution overlay for comparison\n",
    "                x = np.linspace(clean_data.min(), clean_data.max(), 100)\n",
    "                normal_dist = stats.norm.pdf(x, mean_val, std_val)\n",
    "                axes[i].plot(x, normal_dist, 'orange', linewidth=2, alpha=0.8, \n",
    "                            label='Normal Fit')\n",
    "                \n",
    "                axes[i].set_title(f'{var}\\nSkewness: {clean_data.skew():.2f}, '\n",
    "                                f'Outliers Removed: {len(data) - len(clean_data):,}', \n",
    "                                fontsize=11)\n",
    "                axes[i].set_xlabel(var)\n",
    "                axes[i].set_ylabel('Density')\n",
    "                axes[i].legend(fontsize=9)\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(available_key_vars), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Phase 1: Distribution Analysis of Key Loan Variables', \n",
    "                     fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Box plot analysis for outlier detection\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, var in enumerate(available_key_vars):\n",
    "            if var in df_loans.columns and df_loans[var].notna().sum() > 0:\n",
    "                data = df_loans[var].dropna()\n",
    "                \n",
    "                # Create box plot\n",
    "                box_parts = axes[i].boxplot(data, patch_artist=True, \n",
    "                                          boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                                          medianprops=dict(color='red', linewidth=2),\n",
    "                                          flierprops=dict(marker='o', markerfacecolor='red', \n",
    "                                                        markersize=4, alpha=0.5))\n",
    "                \n",
    "                # Add statistical annotations\n",
    "                Q1, Q2, Q3 = data.quantile([0.25, 0.5, 0.75])\n",
    "                IQR = Q3 - Q1\n",
    "                outlier_count = len(data[(data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)])\n",
    "                outlier_pct = (outlier_count / len(data)) * 100\n",
    "                \n",
    "                axes[i].set_title(f'{var}\\nOutliers: {outlier_count:,} ({outlier_pct:.1f}%)\\n'\n",
    "                                f'IQR: {IQR:.2f}', fontsize=11)\n",
    "                axes[i].set_ylabel('Value')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add quartile labels\n",
    "                axes[i].text(1.1, Q1, f'Q1: {Q1:.1f}', fontsize=9, ha='left')\n",
    "                axes[i].text(1.1, Q2, f'Q2: {Q2:.1f}', fontsize=9, ha='left', color='red')\n",
    "                axes[i].text(1.1, Q3, f'Q3: {Q3:.1f}', fontsize=9, ha='left')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(available_key_vars), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Phase 1: Box Plot Analysis - Outlier Detection', \n",
    "                     fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Distribution insights summary\n",
    "        print(f\"\\\\n=== PHASE 1 INSIGHTS ===\")\n",
    "        for var in available_key_vars[:3]:  # Top 3 variables\n",
    "            if var in df_loans.columns and df_loans[var].notna().sum() > 0:\n",
    "                data = df_loans[var].dropna()\n",
    "                skewness = data.skew()\n",
    "                kurtosis = data.kurtosis()\n",
    "                cv = data.std() / data.mean() if data.mean() != 0 else 0\n",
    "                \n",
    "                print(f\"\\\\n{var.upper()}:\")\n",
    "                print(f\"  Distribution: {'Right-skewed' if skewness > 1 else 'Left-skewed' if skewness < -1 else 'Approximately normal'}\")\n",
    "                print(f\"  Variability: {'High' if cv > 1 else 'Moderate' if cv > 0.5 else 'Low'} (CV: {cv:.2f})\")\n",
    "                print(f\"  Tail behavior: {'Heavy-tailed' if kurtosis > 3 else 'Light-tailed' if kurtosis < -1 else 'Normal-tailed'}\")\n",
    "                print(f\"  Data quality: {'Excellent' if skewness < 2 and cv < 1 else 'Good' if skewness < 3 else 'Needs transformation'}\")\n",
    "        \n",
    "        print(f\"\\\\nPhase 1 completed: {len(available_key_vars)} variables analyzed with distribution visualizations\")\n",
    "    else:\n",
    "        print(\"No suitable variables found for Phase 1 visualization\")\n",
    "else:\n",
    "    print(\"Dataset not available for Phase 1 visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Categorical Variables Visualization Implementation\n",
    "if df_loans is not None and len(analysis_categorical_vars) > 0:\n",
    "    print(\"=== CATEGORICAL VARIABLES VISUALIZATION ==\")\n",
    "    \n",
    "    # Enhanced categorical visualization with business insights\n",
    "    key_categorical_vars = ['grade', 'purpose', 'emp_length', 'home_ownership', 'verification_status']\n",
    "    available_cat_vars = [var for var in key_categorical_vars if var in df_loans.columns]\n",
    "    \n",
    "    if available_cat_vars:\n",
    "        print(f\"Analyzing categorical variables: {available_cat_vars}\")\n",
    "        \n",
    "        # 1. Comprehensive categorical distribution analysis\n",
    "        for i, var in enumerate(available_cat_vars[:4]):  # Top 4 most important\n",
    "            value_counts = df_loans[var].value_counts().head(10)  # Top 10 categories\n",
    "            \n",
    "            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            \n",
    "            # A. Enhanced bar chart with percentage annotations\n",
    "            colors = plt.cm.Set3(np.linspace(0, 1, len(value_counts)))\n",
    "            bars = ax1.bar(range(len(value_counts)), value_counts.values, \n",
    "                          color=colors, alpha=0.8, edgecolor='navy', linewidth=1)\n",
    "            \n",
    "            # Add percentage labels on bars\n",
    "            total = value_counts.sum()\n",
    "            for bar, count in zip(bars, value_counts.values):\n",
    "                height = bar.get_height()\n",
    "                percentage = (count / total) * 100\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                        f'{count:,}\\\\n({percentage:.1f}%)', ha='center', va='bottom', \n",
    "                        fontsize=9, fontweight='bold')\n",
    "            \n",
    "            ax1.set_title(f'{var.upper()} - Distribution Analysis', fontsize=14, fontweight='bold')\n",
    "            ax1.set_xlabel('Categories', fontsize=12)\n",
    "            ax1.set_ylabel('Frequency', fontsize=12)\n",
    "            ax1.set_xticks(range(len(value_counts)))\n",
    "            ax1.set_xticklabels([str(x)[:12] + '...' if len(str(x)) > 12 else str(x) \n",
    "                               for x in value_counts.index], rotation=45, ha='right')\n",
    "            ax1.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # B. Pie chart with better styling\n",
    "            wedges, texts, autotexts = ax2.pie(value_counts.values[:8], \n",
    "                                             labels=[str(x)[:8] + '...' if len(str(x)) > 8 else str(x) \n",
    "                                                   for x in value_counts.index[:8]],\n",
    "                                             autopct='%1.1f%%', colors=colors[:8], \n",
    "                                             startangle=90, explode=[0.05]*len(value_counts[:8]))\n",
    "            ax2.set_title(f'{var.upper()} - Proportion (Top 8)', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # C. Relationship with loan amount (if numeric variable available)\n",
    "            if 'loan_amnt' in df_loans.columns:\n",
    "                # Box plot showing loan amount distribution by category\n",
    "                categories_for_box = value_counts.index[:6]  # Top 6 categories for readability\n",
    "                box_data = [df_loans[df_loans[var] == cat]['loan_amnt'].dropna() \n",
    "                           for cat in categories_for_box]\n",
    "                \n",
    "                box_parts = ax3.boxplot(box_data, labels=[str(x)[:8] for x in categories_for_box],\n",
    "                                       patch_artist=True, showfliers=False)\n",
    "                \n",
    "                # Color the boxes\n",
    "                for patch, color in zip(box_parts['boxes'], colors[:6]):\n",
    "                    patch.set_facecolor(color)\n",
    "                    patch.set_alpha(0.8)\n",
    "                \n",
    "                ax3.set_title(f'Loan Amount by {var.upper()}', fontsize=14, fontweight='bold')\n",
    "                ax3.set_xlabel(f'{var} Categories', fontsize=12)\n",
    "                ax3.set_ylabel('Loan Amount ($)', fontsize=12)\n",
    "                ax3.tick_params(axis='x', rotation=45)\n",
    "                ax3.grid(True, alpha=0.3, axis='y')\n",
    "            else:\n",
    "                ax3.text(0.5, 0.5, 'Loan amount\\\\nnot available', ha='center', va='center',\n",
    "                        fontsize=14, transform=ax3.transAxes)\n",
    "                ax3.set_title('Loan Amount Analysis', fontsize=14)\n",
    "            \n",
    "            # D. Statistical summary\n",
    "            ax4.axis('off')\n",
    "            \n",
    "            # Calculate statistics\n",
    "            unique_count = df_loans[var].nunique()\n",
    "            missing_count = df_loans[var].isnull().sum()\n",
    "            missing_pct = (missing_count / len(df_loans)) * 100\n",
    "            top_category = value_counts.index[0]\n",
    "            top_percentage = (value_counts.iloc[0] / total) * 100\n",
    "            \n",
    "            # Concentration analysis\n",
    "            top_3_pct = (value_counts.head(3).sum() / total) * 100\n",
    "            concentration_level = 'High' if top_3_pct > 70 else 'Moderate' if top_3_pct > 50 else 'Low'\n",
    "            \n",
    "            summary_text = f\\\"\\\"\\\"STATISTICAL SUMMARY\n",
    "            \n",
    "Variable: {var.upper()}\n",
    "Total Records: {len(df_loans):,}\n",
    "Unique Categories: {unique_count:,}\n",
    "Missing Values: {missing_count:,} ({missing_pct:.1f}%)\n",
    "\n",
    "TOP CATEGORY ANALYSIS\n",
    "Most Common: '{top_category}'\n",
    "Frequency: {value_counts.iloc[0]:,} ({top_percentage:.1f}%)\n",
    "\n",
    "CONCENTRATION ANALYSIS\n",
    "Top 3 Categories: {top_3_pct:.1f}% of data\n",
    "Concentration Level: {concentration_level}\n",
    "\n",
    "DATA QUALITY\n",
    "Completeness: {100-missing_pct:.1f}%\n",
    "Diversity: {'High' if unique_count > 10 else 'Moderate' if unique_count > 5 else 'Low'}\n",
    "Analysis Suitability: {'Excellent' if missing_pct < 5 and unique_count > 2 else 'Good' if missing_pct < 15 else 'Fair'}\\\"\\\"\\\"\n",
    "            \n",
    "            ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=11,\n",
    "                    verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # 2. Cross-category analysis (if multiple categorical variables available)\n",
    "        if len(available_cat_vars) >= 2:\n",
    "            print(f\"\\\\n=== CROSS-CATEGORICAL ANALYSIS ===\")\n",
    "            \n",
    "            var1, var2 = available_cat_vars[0], available_cat_vars[1]\n",
    "            \n",
    "            # Create contingency table\n",
    "            contingency_table = pd.crosstab(df_loans[var1], df_loans[var2])\n",
    "            \n",
    "            # Select top categories for better visualization\n",
    "            top_var1_cats = df_loans[var1].value_counts().head(5).index\n",
    "            top_var2_cats = df_loans[var2].value_counts().head(5).index\n",
    "            \n",
    "            filtered_contingency = contingency_table.loc[top_var1_cats, top_var2_cats]\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "            \n",
    "            # Heatmap of contingency table\n",
    "            sns.heatmap(filtered_contingency, annot=True, fmt='d', cmap='Blues', \n",
    "                       ax=ax1, cbar_kws={'label': 'Count'})\n",
    "            ax1.set_title(f'Cross-tabulation: {var1.upper()} vs {var2.upper()}', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "            ax1.set_xlabel(f'{var2.upper()}', fontsize=12)\n",
    "            ax1.set_ylabel(f'{var1.upper()}', fontsize=12)\n",
    "            \n",
    "            # Stacked bar chart showing proportions\n",
    "            contingency_pct = filtered_contingency.div(filtered_contingency.sum(axis=1), axis=0) * 100\n",
    "            contingency_pct.plot(kind='bar', stacked=True, ax=ax2, \n",
    "                                colormap='Set3', alpha=0.8)\n",
    "            ax2.set_title(f'Proportion Analysis: {var1.upper()} composition by {var2.upper()}', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "            ax2.set_xlabel(f'{var1.upper()}', fontsize=12)\n",
    "            ax2.set_ylabel('Percentage (%)', fontsize=12)\n",
    "            ax2.legend(title=f'{var2.upper()}', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            ax2.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Chi-square test for independence\n",
    "            from scipy.stats import chi2_contingency\n",
    "            chi2, p_value, dof, expected = chi2_contingency(filtered_contingency)\n",
    "            \n",
    "            print(f\"Chi-square Test Results ({var1} vs {var2}):\")\n",
    "            print(f\"  Chi-square statistic: {chi2:.4f}\")\n",
    "            print(f\"  P-value: {p_value:.4f}\")\n",
    "            print(f\"  Degrees of freedom: {dof}\")\n",
    "            print(f\"  Independence: {'Rejected' if p_value < 0.05 else 'Not rejected'} (Î± = 0.05)\")\n",
    "            print(f\"  Interpretation: {'Variables are dependent' if p_value < 0.05 else 'Variables appear independent'}\")\n",
    "        \n",
    "        # 3. Business insights summary\n",
    "        print(f\"\\\\n=== CATEGORICAL VARIABLES BUSINESS INSIGHTS ===\")\n",
    "        \n",
    "        insights = []\n",
    "        for var in available_cat_vars[:3]:\n",
    "            if var in df_loans.columns:\n",
    "                value_counts = df_loans[var].value_counts()\n",
    "                top_cat = value_counts.index[0]\n",
    "                top_pct = (value_counts.iloc[0] / len(df_loans)) * 100\n",
    "                \n",
    "                if var == 'grade' and top_pct > 30:\n",
    "                    insights.append(f\"Risk concentration: {top_pct:.1f}% of loans in grade '{top_cat}'\")\n",
    "                elif var == 'purpose' and 'debt_consolidation' in str(top_cat).lower():\n",
    "                    insights.append(f\"Debt consolidation dominates: {top_pct:.1f}% of loan purposes\")\n",
    "                elif var == 'home_ownership' and 'rent' in str(top_cat).lower():\n",
    "                    insights.append(f\"Housing risk: {top_pct:.1f}% are renters, affecting collateral\")\n",
    "                else:\n",
    "                    insights.append(f\"{var.capitalize()} pattern: '{top_cat}' represents {top_pct:.1f}% of cases\")\n",
    "        \n",
    "        for i, insight in enumerate(insights, 1):\n",
    "            print(f\"  {i}. {insight}\")\n",
    "        \n",
    "        print(f\"\\\\nCategorical analysis completed for {len(available_cat_vars)} variables\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No suitable categorical variables found for visualization\")\n",
    "else:\n",
    "    print(\"Dataset or categorical variables not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis and Relationships\n",
    "\n",
    "Comprehensive correlation analysis to identify relationships between variables and potential patterns for research question development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Advanced Analytical Visualizations Implementation\n",
    "if df_loans is not None:\n",
    "    print(\"=== ADVANCED ANALYTICAL VISUALIZATIONS ==\")\n",
    "    \n",
    "    # 1. Time Series Analysis (if date columns are available)\n",
    "    date_columns = [col for col in df_loans.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "    \n",
    "    if date_columns:\n",
    "        print(f\"Time series analysis for: {date_columns}\")\n",
    "        \n",
    "        for date_col in date_columns[:2]:  # Analyze first 2 date columns\n",
    "            if df_loans[date_col].notna().sum() > 100:  # Sufficient non-null dates\n",
    "                \n",
    "                # Convert to datetime if not already\n",
    "                df_loans[f'{date_col}_parsed'] = pd.to_datetime(df_loans[date_col], errors='coerce')\n",
    "                \n",
    "                # Extract time components\n",
    "                df_loans['year'] = df_loans[f'{date_col}_parsed'].dt.year\n",
    "                df_loans['month'] = df_loans[f'{date_col}_parsed'].dt.month\n",
    "                df_loans['quarter'] = df_loans[f'{date_col}_parsed'].dt.quarter\n",
    "                \n",
    "                fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "                \n",
    "                # A. Loan volume over time (yearly)\n",
    "                yearly_counts = df_loans.groupby('year').size()\n",
    "                ax1.plot(yearly_counts.index, yearly_counts.values, marker='o', \n",
    "                        linewidth=3, markersize=8, color='blue', alpha=0.8)\n",
    "                ax1.fill_between(yearly_counts.index, yearly_counts.values, alpha=0.3, color='blue')\n",
    "                ax1.set_title(f'Loan Volume Trend by Year\\\\n({date_col})', \n",
    "                             fontsize=14, fontweight='bold')\n",
    "                ax1.set_xlabel('Year')\n",
    "                ax1.set_ylabel('Number of Loans')\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add trend annotation\n",
    "                if len(yearly_counts) > 1:\n",
    "                    trend = 'Increasing' if yearly_counts.iloc[-1] > yearly_counts.iloc[0] else 'Decreasing'\n",
    "                    ax1.text(0.7, 0.9, f'Trend: {trend}', transform=ax1.transAxes,\n",
    "                            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "                \n",
    "                # B. Average loan amount over time\n",
    "                if 'loan_amnt' in df_loans.columns:\n",
    "                    yearly_avg_amount = df_loans.groupby('year')['loan_amnt'].mean()\n",
    "                    ax2.plot(yearly_avg_amount.index, yearly_avg_amount.values, \n",
    "                            marker='s', linewidth=3, markersize=8, color='green', alpha=0.8)\n",
    "                    ax2.fill_between(yearly_avg_amount.index, yearly_avg_amount.values, \n",
    "                                   alpha=0.3, color='green')\n",
    "                    ax2.set_title('Average Loan Amount Trend by Year', \n",
    "                                 fontsize=14, fontweight='bold')\n",
    "                    ax2.set_xlabel('Year')\n",
    "                    ax2.set_ylabel('Average Loan Amount ($)')\n",
    "                    ax2.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # Format y-axis as currency\n",
    "                    ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n",
    "                \n",
    "                # C. Monthly seasonality analysis\n",
    "                monthly_counts = df_loans.groupby('month').size()\n",
    "                month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "                colors = plt.cm.Set3(np.linspace(0, 1, 12))\n",
    "                \n",
    "                bars = ax3.bar(monthly_counts.index, monthly_counts.values, \n",
    "                              color=colors, alpha=0.8, edgecolor='navy')\n",
    "                ax3.set_title('Loan Application Seasonality by Month', \n",
    "                             fontsize=14, fontweight='bold')\n",
    "                ax3.set_xlabel('Month')\n",
    "                ax3.set_ylabel('Number of Loans')\n",
    "                ax3.set_xticks(range(1, 13))\n",
    "                ax3.set_xticklabels(month_names, rotation=45)\n",
    "                ax3.grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                # Highlight peak and low months\n",
    "                peak_month = monthly_counts.idxmax()\n",
    "                low_month = monthly_counts.idxmin()\n",
    "                ax3.text(peak_month, monthly_counts[peak_month] + monthly_counts.max()*0.05,\n",
    "                        f'Peak\\\\n({month_names[peak_month-1]})', ha='center', fontweight='bold', color='red')\n",
    "                ax3.text(low_month, monthly_counts[low_month] + monthly_counts.max()*0.05,\n",
    "                        f'Low\\\\n({month_names[low_month-1]})', ha='center', fontweight='bold', color='blue')\n",
    "                \n",
    "                # D. Quarterly analysis with business insights\n",
    "                quarterly_data = df_loans.groupby(['year', 'quarter']).agg({\n",
    "                    'loan_amnt': ['count', 'mean', 'sum'] if 'loan_amnt' in df_loans.columns else {'year': 'count'}\n",
    "                }).reset_index()\n",
    "                \n",
    "                if 'loan_amnt' in df_loans.columns:\n",
    "                    quarterly_data.columns = ['year', 'quarter', 'count', 'avg_amount', 'total_volume']\n",
    "                    quarterly_data['quarter_label'] = quarterly_data.apply(lambda x: f\"{int(x['year'])}-Q{int(x['quarter'])}\", axis=1)\n",
    "                    \n",
    "                    # Plot quarterly total volume\n",
    "                    ax4.bar(range(len(quarterly_data)), quarterly_data['total_volume'], \n",
    "                           alpha=0.7, color='orange', edgecolor='navy')\n",
    "                    ax4.set_title('Quarterly Total Loan Volume', fontsize=14, fontweight='bold')\n",
    "                    ax4.set_xlabel('Quarter')\n",
    "                    ax4.set_ylabel('Total Loan Volume ($)')\n",
    "                    ax4.set_xticks(range(0, len(quarterly_data), max(1, len(quarterly_data)//8)))\n",
    "                    ax4.set_xticklabels([quarterly_data.iloc[i]['quarter_label'] \n",
    "                                       for i in range(0, len(quarterly_data), max(1, len(quarterly_data)//8))], \n",
    "                                       rotation=45)\n",
    "                    ax4.grid(True, alpha=0.3, axis='y')\n",
    "                    ax4.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1e6:.1f}M'))\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Time series insights\n",
    "                print(f\"\\\\n=== TIME SERIES INSIGHTS ===\")\n",
    "                print(f\"Analysis period: {df_loans['year'].min()}-{df_loans['year'].max()}\")\n",
    "                print(f\"Peak application month: {month_names[monthly_counts.idxmax()-1]} ({monthly_counts.max():,} loans)\")\n",
    "                print(f\"Lowest application month: {month_names[monthly_counts.idxmin()-1]} ({monthly_counts.min():,} loans)\")\n",
    "                \n",
    "                if len(yearly_counts) > 1:\n",
    "                    growth_rate = ((yearly_counts.iloc[-1] - yearly_counts.iloc[0]) / yearly_counts.iloc[0]) * 100\n",
    "                    print(f\"Overall growth rate: {growth_rate:+.1f}% from {yearly_counts.index[0]} to {yearly_counts.index[-1]}\")\n",
    "    \n",
    "    # 2. Geographic Analysis (if state or address information available)\n",
    "    geo_columns = [col for col in df_loans.columns if 'state' in col.lower() or 'addr' in col.lower()]\n",
    "    \n",
    "    if geo_columns:\n",
    "        print(f\"\\\\n=== GEOGRAPHIC ANALYSIS ===\")\n",
    "        \n",
    "        geo_col = geo_columns[0]  # Use first geographic column\n",
    "        \n",
    "        if df_loans[geo_col].notna().sum() > 50:  # Sufficient geographic data\n",
    "            \n",
    "            # State-level analysis\n",
    "            state_analysis = df_loans.groupby(geo_col).agg({\n",
    "                'loan_amnt': ['count', 'mean', 'sum'] if 'loan_amnt' in df_loans.columns else {geo_col: 'count'}\n",
    "            }).reset_index()\n",
    "            \n",
    "            if 'loan_amnt' in df_loans.columns:\n",
    "                state_analysis.columns = [geo_col, 'loan_count', 'avg_loan_amount', 'total_volume']\n",
    "                state_analysis = state_analysis.sort_values('loan_count', ascending=False).head(15)\n",
    "                \n",
    "                fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\n",
    "                \n",
    "                # A. Top states by loan count\n",
    "                bars1 = ax1.bar(range(len(state_analysis)), state_analysis['loan_count'], \n",
    "                               color='lightblue', alpha=0.8, edgecolor='navy')\n",
    "                ax1.set_title('Top 15 States by Loan Count', fontsize=14, fontweight='bold')\n",
    "                ax1.set_xlabel('State')\n",
    "                ax1.set_ylabel('Number of Loans')\n",
    "                ax1.set_xticks(range(len(state_analysis)))\n",
    "                ax1.set_xticklabels(state_analysis[geo_col], rotation=45)\n",
    "                ax1.grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                # Add value labels\n",
    "                for bar, count in zip(bars1, state_analysis['loan_count']):\n",
    "                    height = bar.get_height()\n",
    "                    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                            f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
    "                \n",
    "                # B. Average loan amount by state\n",
    "                bars2 = ax2.bar(range(len(state_analysis)), state_analysis['avg_loan_amount'], \n",
    "                               color='lightgreen', alpha=0.8, edgecolor='navy')\n",
    "                ax2.set_title('Average Loan Amount by Top States', fontsize=14, fontweight='bold')\n",
    "                ax2.set_xlabel('State')\n",
    "                ax2.set_ylabel('Average Loan Amount ($)')\n",
    "                ax2.set_xticks(range(len(state_analysis)))\n",
    "                ax2.set_xticklabels(state_analysis[geo_col], rotation=45)\n",
    "                ax2.grid(True, alpha=0.3, axis='y')\n",
    "                ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n",
    "                \n",
    "                # C. Market share analysis (pie chart for top 10)\n",
    "                top_10_states = state_analysis.head(10)\n",
    "                others_count = state_analysis.iloc[10:]['loan_count'].sum() if len(state_analysis) > 10 else 0\n",
    "                \n",
    "                if others_count > 0:\n",
    "                    pie_data = list(top_10_states['loan_count']) + [others_count]\n",
    "                    pie_labels = list(top_10_states[geo_col]) + ['Others']\n",
    "                else:\n",
    "                    pie_data = top_10_states['loan_count']\n",
    "                    pie_labels = top_10_states[geo_col]\n",
    "                \n",
    "                colors = plt.cm.Set3(np.linspace(0, 1, len(pie_data)))\n",
    "                wedges, texts, autotexts = ax3.pie(pie_data, labels=pie_labels, \n",
    "                                                  autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "                ax3.set_title('Market Share by State (Top 10)', fontsize=14, fontweight='bold')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Geographic insights\n",
    "                print(f\"Geographic distribution insights:\")\n",
    "                print(f\"  Top state: {state_analysis.iloc[0][geo_col]} ({state_analysis.iloc[0]['loan_count']:,} loans)\")\n",
    "                print(f\"  States analyzed: {len(state_analysis)}\")\n",
    "                print(f\"  Market concentration: Top 3 states represent {(state_analysis.head(3)['loan_count'].sum() / state_analysis['loan_count'].sum() * 100):.1f}% of loans\")\n",
    "    \n",
    "    # 3. Multi-dimensional analysis\n",
    "    if 'loan_amnt' in df_loans.columns and len(analysis_numeric_vars) >= 3:\n",
    "        print(f\"\\\\n=== MULTI-DIMENSIONAL ANALYSIS ===\")\n",
    "        \n",
    "        # Select key variables for 3D analysis\n",
    "        key_vars = ['loan_amnt', 'annual_inc', 'int_rate'][:3]\n",
    "        available_vars = [var for var in key_vars if var in df_loans.columns]\n",
    "        \n",
    "        if len(available_vars) >= 3:\n",
    "            # Sample data to avoid overplotting\n",
    "            sample_data = df_loans[available_vars + ['grade'] if 'grade' in df_loans.columns else available_vars].dropna().sample(\n",
    "                min(2000, len(df_loans)), random_state=42)\n",
    "            \n",
    "            fig = plt.figure(figsize=(18, 6))\n",
    "            \n",
    "            # A. 3D scatter plot\n",
    "            ax1 = fig.add_subplot(131, projection='3d')\n",
    "            \n",
    "            if 'grade' in df_loans.columns:\n",
    "                # Color by grade\n",
    "                grades = sample_data['grade'].unique()[:7]  # Top 7 grades\n",
    "                colors = plt.cm.Set1(np.linspace(0, 1, len(grades)))\n",
    "                \n",
    "                for i, grade in enumerate(grades):\n",
    "                    grade_data = sample_data[sample_data['grade'] == grade]\n",
    "                    ax1.scatter(grade_data[available_vars[0]], \n",
    "                              grade_data[available_vars[1]], \n",
    "                              grade_data[available_vars[2]],\n",
    "                              c=[colors[i]], alpha=0.6, s=20, label=f'Grade {grade}')\n",
    "                \n",
    "                ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            else:\n",
    "                ax1.scatter(sample_data[available_vars[0]], \n",
    "                          sample_data[available_vars[1]], \n",
    "                          sample_data[available_vars[2]], \n",
    "                          c='blue', alpha=0.6, s=20)\n",
    "            \n",
    "            ax1.set_xlabel(available_vars[0])\n",
    "            ax1.set_ylabel(available_vars[1])\n",
    "            ax1.set_zlabel(available_vars[2])\n",
    "            ax1.set_title('3D Relationship Analysis', fontsize=12, fontweight='bold')\n",
    "            \n",
    "            # B. Correlation network (if plotly is not available, use simplified version)\n",
    "            ax2 = fig.add_subplot(132)\n",
    "            \n",
    "            # Create correlation matrix for available numeric variables\n",
    "            corr_data = df_loans[analysis_numeric_vars[:8]].corr()\n",
    "            \n",
    "            # Create network-style visualization\n",
    "            G_pos = {}  # Simple circular layout\n",
    "            n_vars = len(corr_data.columns)\n",
    "            for i, var in enumerate(corr_data.columns):\n",
    "                angle = 2 * np.pi * i / n_vars\n",
    "                G_pos[var] = (np.cos(angle), np.sin(angle))\n",
    "            \n",
    "            # Draw strong correlations as connections\n",
    "            for i, var1 in enumerate(corr_data.columns):\n",
    "                for j, var2 in enumerate(corr_data.columns):\n",
    "                    if i < j and abs(corr_data.iloc[i, j]) > 0.5:\n",
    "                        x_coords = [G_pos[var1][0], G_pos[var2][0]]\n",
    "                        y_coords = [G_pos[var1][1], G_pos[var2][1]]\n",
    "                        \n",
    "                        # Line thickness based on correlation strength\n",
    "                        thickness = abs(corr_data.iloc[i, j]) * 5\n",
    "                        color = 'red' if corr_data.iloc[i, j] > 0 else 'blue'\n",
    "                        ax2.plot(x_coords, y_coords, color=color, alpha=0.6, linewidth=thickness)\n",
    "            \n",
    "            # Draw nodes\n",
    "            for var, (x, y) in G_pos.items():\n",
    "                ax2.scatter(x, y, s=300, c='lightblue', edgecolor='navy', alpha=0.8)\n",
    "                ax2.text(x, y, var[:8], ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "            \n",
    "            ax2.set_title('Variable Correlation Network\\\\n(|r| > 0.5)', fontsize=12, fontweight='bold')\n",
    "            ax2.set_xlim(-1.5, 1.5)\n",
    "            ax2.set_ylim(-1.5, 1.5)\n",
    "            ax2.axis('off')\n",
    "            \n",
    "            # C. Risk stratification analysis\n",
    "            ax3 = fig.add_subplot(133)\n",
    "            \n",
    "            if 'grade' in df_loans.columns and 'int_rate' in df_loans.columns:\n",
    "                # Risk analysis by grade and interest rate\n",
    "                risk_data = df_loans.groupby('grade').agg({\n",
    "                    'int_rate': ['mean', 'count'],\n",
    "                    'loan_amnt': 'mean' if 'loan_amnt' in df_loans.columns else 'count'\n",
    "                }).reset_index()\n",
    "                \n",
    "                risk_data.columns = ['grade', 'avg_int_rate', 'count', 'avg_loan_amnt']\n",
    "                risk_data = risk_data.sort_values('avg_int_rate')\n",
    "                \n",
    "                # Bubble chart: x=grade, y=interest rate, size=count, color=loan amount\n",
    "                scatter = ax3.scatter(range(len(risk_data)), risk_data['avg_int_rate'], \n",
    "                                    s=risk_data['count']/10, c=risk_data['avg_loan_amnt'], \n",
    "                                    cmap='viridis', alpha=0.7, edgecolors='black')\n",
    "                \n",
    "                ax3.set_title('Risk Stratification Analysis\\\\n(Size=Count, Color=Avg Amount)', \n",
    "                             fontsize=12, fontweight='bold')\n",
    "                ax3.set_xlabel('Loan Grade')\n",
    "                ax3.set_ylabel('Average Interest Rate (%)')\n",
    "                ax3.set_xticks(range(len(risk_data)))\n",
    "                ax3.set_xticklabels(risk_data['grade'])\n",
    "                ax3.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add colorbar\n",
    "                plt.colorbar(scatter, ax=ax3, label='Avg Loan Amount ($)')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Multi-dimensional analysis completed for {len(available_vars)} key variables\")\n",
    "        \n",
    "    print(f\"\\\\nAdvanced analytical visualizations completed\")\n",
    "    \n",
    "else:\n",
    "    print(\"Dataset not available for advanced analytical visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: Research Question Specific Visualizations\n",
    "# Based on EDA findings, create visualizations for specific research questions\n",
    "\n",
    "if df_loans is not None:\n",
    "    print(\"=== RESEARCH QUESTION SPECIFIC VISUALIZATIONS ==\")\n",
    "    \n",
    "    # Research questions identified from EDA analysis\n",
    "    research_questions = [\n",
    "        \"How do loan characteristics vary by risk grade and what drives interest rate pricing?\",\n",
    "        \"What are the key factors that influence loan approval and default patterns?\", \n",
    "        \"How does borrower profile (income, employment, credit) impact loan terms?\",\n",
    "        \"What geographic and temporal patterns exist in lending behavior?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\\\nResearch Questions for Visualization:\")\n",
    "    for i, rq in enumerate(research_questions, 1):\n",
    "        print(f\"  {i}. {rq}\")\n",
    "    \n",
    "    # Question 1: Risk Grade Analysis and Interest Rate Pricing\n",
    "    print(f\"\\\\n=== RESEARCH QUESTION 1: RISK-BASED PRICING ANALYSIS ===\")\n",
    "    if 'grade' in df_loans.columns and 'int_rate' in df_loans.columns:\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        \n",
    "        # A. Risk grade distribution with loan volume\n",
    "        grade_analysis = df_loans.groupby('grade').agg({\n",
    "            'loan_amnt': ['count', 'mean', 'sum'],\n",
    "            'int_rate': ['mean', 'std'],\n",
    "            'annual_inc': 'mean' if 'annual_inc' in df_loans.columns else 'count'\n",
    "        }).round(2)\n",
    "        \n",
    "        grade_analysis.columns = ['loan_count', 'avg_loan_amnt', 'total_volume', \n",
    "                                'avg_int_rate', 'int_rate_std', 'avg_income']\n",
    "        grade_analysis = grade_analysis.reset_index().sort_values('avg_int_rate')\n",
    "        \n",
    "        # Dual-axis plot: loan count and interest rate\n",
    "        ax1_twin = ax1.twinx()\n",
    "        \n",
    "        bars1 = ax1.bar(grade_analysis['grade'], grade_analysis['loan_count'], \n",
    "                       alpha=0.6, color='lightblue', label='Loan Count')\n",
    "        line1 = ax1_twin.plot(grade_analysis['grade'], grade_analysis['avg_int_rate'], \n",
    "                             color='red', marker='o', linewidth=3, markersize=8, \n",
    "                             label='Avg Interest Rate')\n",
    "        \n",
    "        ax1.set_xlabel('Risk Grade')\n",
    "        ax1.set_ylabel('Loan Count', color='blue')\n",
    "        ax1_twin.set_ylabel('Average Interest Rate (%)', color='red')\n",
    "        ax1.set_title('Risk Grade Distribution vs Interest Rate Pricing', fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Combined legend\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax1_twin.get_legend_handles_labels()\n",
    "        ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "        \n",
    "        # B. Interest rate spread by grade (box plot)\n",
    "        grade_int_data = [df_loans[df_loans['grade'] == grade]['int_rate'].dropna() \n",
    "                         for grade in sorted(df_loans['grade'].unique())]\n",
    "        \n",
    "        box_parts = ax2.boxplot(grade_int_data, labels=sorted(df_loans['grade'].unique()),\n",
    "                               patch_artist=True, showfliers=True)\n",
    "        \n",
    "        # Color boxes by risk level\n",
    "        colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(box_parts['boxes'])))\n",
    "        for patch, color in zip(box_parts['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.8)\n",
    "        \n",
    "        ax2.set_xlabel('Risk Grade')\n",
    "        ax2.set_ylabel('Interest Rate (%)')\n",
    "        ax2.set_title('Interest Rate Distribution by Risk Grade', fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # C. Risk-Return Profile (scatter: loan amount vs interest rate by grade)\n",
    "        if 'loan_amnt' in df_loans.columns:\n",
    "            sample_data = df_loans[['grade', 'loan_amnt', 'int_rate']].dropna().sample(min(3000, len(df_loans)), random_state=42)\n",
    "            \n",
    "            grades = sorted(sample_data['grade'].unique())\n",
    "            colors = plt.cm.Set1(np.linspace(0, 1, len(grades)))\n",
    "            \n",
    "            for i, grade in enumerate(grades):\n",
    "                grade_data = sample_data[sample_data['grade'] == grade]\n",
    "                ax3.scatter(grade_data['loan_amnt'], grade_data['int_rate'], \n",
    "                           c=[colors[i]], alpha=0.6, s=30, label=f'Grade {grade}')\n",
    "            \n",
    "            ax3.set_xlabel('Loan Amount ($)')\n",
    "            ax3.set_ylabel('Interest Rate (%)')\n",
    "            ax3.set_title('Loan Amount vs Interest Rate by Risk Grade', fontweight='bold')\n",
    "            ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            ax3.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "        \n",
    "        # D. Pricing efficiency analysis\n",
    "        if len(grade_analysis) > 5:\n",
    "            # Calculate pricing spread between grades\n",
    "            grade_analysis['spread_from_prev'] = grade_analysis['avg_int_rate'].diff()\n",
    "            \n",
    "            ax4.bar(range(1, len(grade_analysis)), grade_analysis['spread_from_prev'][1:], \n",
    "                   alpha=0.7, color='orange', edgecolor='navy')\n",
    "            ax4.set_xlabel('Grade Transition')\n",
    "            ax4.set_ylabel('Interest Rate Spread (%)')\n",
    "            ax4.set_title('Risk-Based Pricing Spreads Between Grades', fontweight='bold')\n",
    "            ax4.set_xticks(range(1, len(grade_analysis)))\n",
    "            ax4.set_xticklabels([f'{grade_analysis.iloc[i-1][\"grade\"]}â{grade_analysis.iloc[i][\"grade\"]}' \n",
    "                               for i in range(1, len(grade_analysis))], rotation=45)\n",
    "            ax4.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add average spread line\n",
    "            avg_spread = grade_analysis['spread_from_prev'][1:].mean()\n",
    "            ax4.axhline(y=avg_spread, color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Avg Spread: {avg_spread:.2f}%')\n",
    "            ax4.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Risk pricing insights\n",
    "        print(f\"Risk-Based Pricing Analysis:\")\n",
    "        print(f\"  Lowest risk grade: {grade_analysis.iloc[0]['grade']} (avg rate: {grade_analysis.iloc[0]['avg_int_rate']:.2f}%)\")\n",
    "        print(f\"  Highest risk grade: {grade_analysis.iloc[-1]['grade']} (avg rate: {grade_analysis.iloc[-1]['avg_int_rate']:.2f}%)\")\n",
    "        print(f\"  Rate spread (high-low): {grade_analysis.iloc[-1]['avg_int_rate'] - grade_analysis.iloc[0]['avg_int_rate']:.2f}%\")\n",
    "        print(f\"  Most common grade: {grade_analysis.sort_values('loan_count', ascending=False).iloc[0]['grade']}\")\n",
    "    \n",
    "    # Question 2: Default Risk Factors Analysis\n",
    "    print(f\"\\\\n=== RESEARCH QUESTION 2: DEFAULT RISK FACTORS ===\")\n",
    "    \n",
    "    # Create risk proxy using available variables\n",
    "    risk_factors = ['dti', 'delinq_2yrs', 'inq_last_6mths', 'pub_rec'] if all(col in df_loans.columns for col in ['dti', 'delinq_2yrs', 'inq_last_6mths', 'pub_rec']) else []\n",
    "    \n",
    "    if len(risk_factors) >= 2:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        \n",
    "        # A. DTI distribution by grade (risk indicator)\n",
    "        if 'dti' in df_loans.columns and 'grade' in df_loans.columns:\n",
    "            dti_by_grade = [df_loans[df_loans['grade'] == grade]['dti'].dropna() \n",
    "                           for grade in sorted(df_loans['grade'].unique())[:7]]  # Top 7 grades\n",
    "            \n",
    "            violin_parts = ax1.violinplot(dti_by_grade, positions=range(len(dti_by_grade)), \n",
    "                                        showmeans=True, showmedians=True)\n",
    "            \n",
    "            ax1.set_xlabel('Risk Grade')\n",
    "            ax1.set_ylabel('Debt-to-Income Ratio')\n",
    "            ax1.set_title('DTI Distribution by Risk Grade\\\\n(Higher DTI = Higher Default Risk)', fontweight='bold')\n",
    "            ax1.set_xticks(range(len(dti_by_grade)))\n",
    "            ax1.set_xticklabels(sorted(df_loans['grade'].unique())[:7])\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # B. Delinquency history impact\n",
    "        if 'delinq_2yrs' in df_loans.columns and 'int_rate' in df_loans.columns:\n",
    "            delinq_analysis = df_loans.groupby('delinq_2yrs').agg({\n",
    "                'int_rate': 'mean',\n",
    "                'loan_amnt': 'count'\n",
    "            }).reset_index()\n",
    "            delinq_analysis = delinq_analysis[delinq_analysis['delinq_2yrs'] <= 5]  # Focus on reasonable range\n",
    "            \n",
    "            bars2 = ax2.bar(delinq_analysis['delinq_2yrs'], delinq_analysis['loan_amnt'], \n",
    "                           alpha=0.6, color='lightcoral')\n",
    "            ax2_twin = ax2.twinx()\n",
    "            line2 = ax2_twin.plot(delinq_analysis['delinq_2yrs'], delinq_analysis['int_rate'], \n",
    "                                 color='darkred', marker='s', linewidth=3, markersize=8)\n",
    "            \n",
    "            ax2.set_xlabel('Number of Delinquencies (Past 2 Years)')\n",
    "            ax2.set_ylabel('Loan Count', color='coral')\n",
    "            ax2_twin.set_ylabel('Average Interest Rate (%)', color='darkred')\n",
    "            ax2.set_title('Delinquency History Impact on Pricing', fontweight='bold')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # C. Credit inquiries vs loan characteristics\n",
    "        if 'inq_last_6mths' in df_loans.columns:\n",
    "            inq_analysis = df_loans[df_loans['inq_last_6mths'] <= 10].groupby('inq_last_6mths').agg({\n",
    "                'int_rate': 'mean',\n",
    "                'loan_amnt': ['mean', 'count']\n",
    "            }).reset_index()\n",
    "            inq_analysis.columns = ['inquiries', 'avg_int_rate', 'avg_loan_amnt', 'count']\n",
    "            \n",
    "            # Bubble chart: x=inquiries, y=interest rate, size=count, color=loan amount\n",
    "            scatter3 = ax3.scatter(inq_analysis['inquiries'], inq_analysis['avg_int_rate'], \n",
    "                                  s=inq_analysis['count']/10, c=inq_analysis['avg_loan_amnt'],\n",
    "                                  cmap='plasma', alpha=0.7, edgecolors='black')\n",
    "            \n",
    "            ax3.set_xlabel('Credit Inquiries (Last 6 Months)')\n",
    "            ax3.set_ylabel('Average Interest Rate (%)')\n",
    "            ax3.set_title('Credit Shopping Behavior Impact\\\\n(Size=Count, Color=Avg Amount)', fontweight='bold')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            plt.colorbar(scatter3, ax=ax3, label='Avg Loan Amount ($)')\n",
    "        \n",
    "        # D. Risk factor correlation heatmap\n",
    "        risk_corr = df_loans[risk_factors + ['int_rate'] if 'int_rate' in df_loans.columns else risk_factors].corr()\n",
    "        \n",
    "        mask = np.triu(np.ones_like(risk_corr, dtype=bool))\n",
    "        sns.heatmap(risk_corr, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "                   square=True, ax=ax4, cbar_kws={\"shrink\": .8})\n",
    "        ax4.set_title('Risk Factors Correlation Matrix', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Default Risk Factors Analysis:\")\n",
    "        print(f\"  Risk factors analyzed: {len(risk_factors)}\")\n",
    "        if 'dti' in df_loans.columns:\n",
    "            high_dti_pct = (df_loans['dti'] > 20).sum() / len(df_loans) * 100\n",
    "            print(f\"  High DTI borrowers (>20%): {high_dti_pct:.1f}%\")\n",
    "        if 'delinq_2yrs' in df_loans.columns:\n",
    "            clean_credit_pct = (df_loans['delinq_2yrs'] == 0).sum() / df_loans['delinq_2yrs'].notna().sum() * 100\n",
    "            print(f\"  Clean credit history (0 delinquencies): {clean_credit_pct:.1f}%\")\n",
    "    \n",
    "    # Question 3: Borrower Profile Impact Analysis\n",
    "    print(f\"\\\\n=== RESEARCH QUESTION 3: BORROWER PROFILE IMPACT ===\")\n",
    "    \n",
    "    if 'annual_inc' in df_loans.columns and 'emp_length' in df_loans.columns:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        \n",
    "        # A. Income distribution impact on loan terms\n",
    "        # Create income bins\n",
    "        df_loans['income_bin'] = pd.cut(df_loans['annual_inc'], bins=5, labels=['Low', 'Low-Mid', 'Middle', 'Mid-High', 'High'])\n",
    "        \n",
    "        income_analysis = df_loans.groupby('income_bin').agg({\n",
    "            'loan_amnt': ['mean', 'count'],\n",
    "            'int_rate': 'mean' if 'int_rate' in df_loans.columns else 'count'\n",
    "        }).reset_index()\n",
    "        income_analysis.columns = ['income_bin', 'avg_loan_amnt', 'count', 'avg_int_rate']\n",
    "        \n",
    "        bars1 = ax1.bar(range(len(income_analysis)), income_analysis['avg_loan_amnt'], \n",
    "                       alpha=0.7, color='lightgreen')\n",
    "        ax1_twin = ax1.twinx()\n",
    "        line1 = ax1_twin.plot(range(len(income_analysis)), income_analysis['avg_int_rate'], \n",
    "                             color='red', marker='o', linewidth=3, markersize=8)\n",
    "        \n",
    "        ax1.set_xlabel('Income Level')\n",
    "        ax1.set_ylabel('Average Loan Amount ($)', color='green')\n",
    "        ax1_twin.set_ylabel('Average Interest Rate (%)', color='red')\n",
    "        ax1.set_title('Income Level Impact on Loan Terms', fontweight='bold')\n",
    "        ax1.set_xticks(range(len(income_analysis)))\n",
    "        ax1.set_xticklabels(income_analysis['income_bin'], rotation=45)\n",
    "        ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # B. Employment length analysis\n",
    "        emp_length_order = ['< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years', \n",
    "                          '6 years', '7 years', '8 years', '9 years', '10+ years']\n",
    "        available_emp_lengths = [emp for emp in emp_length_order if emp in df_loans['emp_length'].unique()]\n",
    "        \n",
    "        if len(available_emp_lengths) > 3:\n",
    "            emp_analysis = df_loans[df_loans['emp_length'].isin(available_emp_lengths)].groupby('emp_length').agg({\n",
    "                'loan_amnt': 'mean',\n",
    "                'int_rate': 'mean' if 'int_rate' in df_loans.columns else 'count'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Sort by employment length order\n",
    "            emp_analysis['emp_order'] = emp_analysis['emp_length'].map({emp: i for i, emp in enumerate(available_emp_lengths)})\n",
    "            emp_analysis = emp_analysis.sort_values('emp_order')\n",
    "            \n",
    "            ax2.plot(range(len(emp_analysis)), emp_analysis['loan_amnt'], \n",
    "                    marker='o', linewidth=3, markersize=8, color='blue', alpha=0.8)\n",
    "            ax2.set_xlabel('Employment Length')\n",
    "            ax2.set_ylabel('Average Loan Amount ($)')\n",
    "            ax2.set_title('Employment Stability vs Loan Amount', fontweight='bold')\n",
    "            ax2.set_xticks(range(len(emp_analysis)))\n",
    "            ax2.set_xticklabels(emp_analysis['emp_length'], rotation=45, ha='right')\n",
    "            ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # C. Purpose vs borrower profile\n",
    "        if 'purpose' in df_loans.columns:\n",
    "            purpose_profile = df_loans.groupby('purpose').agg({\n",
    "                'annual_inc': 'mean',\n",
    "                'loan_amnt': 'mean',\n",
    "                'int_rate': 'mean' if 'int_rate' in df_loans.columns else 'count'\n",
    "            }).reset_index().sort_values('annual_inc', ascending=False).head(8)\n",
    "            \n",
    "            bars3 = ax3.barh(range(len(purpose_profile)), purpose_profile['annual_inc'], \n",
    "                           alpha=0.7, color='lightblue')\n",
    "            ax3.set_ylabel('Loan Purpose')\n",
    "            ax3.set_xlabel('Average Annual Income ($)')\n",
    "            ax3.set_title('Borrower Income Profile by Loan Purpose', fontweight='bold')\n",
    "            ax3.set_yticks(range(len(purpose_profile)))\n",
    "            ax3.set_yticklabels([purpose[:20] + '...' if len(purpose) > 20 else purpose \n",
    "                               for purpose in purpose_profile['purpose']])\n",
    "            ax3.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n",
    "            ax3.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # D. Credit utilization analysis (if available)\n",
    "        credit_factors = ['loan_amnt', 'annual_inc']\n",
    "        if all(col in df_loans.columns for col in credit_factors):\n",
    "            df_loans['loan_to_income'] = df_loans['loan_amnt'] / df_loans['annual_inc']\n",
    "            \n",
    "            # Remove outliers for better visualization\n",
    "            lti_clean = df_loans['loan_to_income'][(df_loans['loan_to_income'] > 0) & (df_loans['loan_to_income'] < 1)]\n",
    "            \n",
    "            ax4.hist(lti_clean, bins=30, alpha=0.7, color='orange', edgecolor='navy')\n",
    "            ax4.axvline(lti_clean.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Mean: {lti_clean.mean():.2f}')\n",
    "            ax4.axvline(lti_clean.median(), color='green', linestyle='--', linewidth=2, \n",
    "                       label=f'Median: {lti_clean.median():.2f}')\n",
    "            ax4.set_xlabel('Loan-to-Income Ratio')\n",
    "            ax4.set_ylabel('Frequency')\n",
    "            ax4.set_title('Loan-to-Income Distribution\\\\n(Credit Utilization Pattern)', fontweight='bold')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Borrower Profile Impact Analysis:\")\n",
    "        if 'income_bin' in df_loans.columns:\n",
    "            mode_income = df_loans['income_bin'].mode()[0]\n",
    "            print(f\"  Most common income level: {mode_income}\")\n",
    "        print(f\"  Employment factors analyzed: {len(available_emp_lengths) if 'available_emp_lengths' in locals() else 0}\")\n",
    "        if 'loan_to_income' in df_loans.columns:\n",
    "            avg_lti = df_loans['loan_to_income'][(df_loans['loan_to_income'] > 0) & (df_loans['loan_to_income'] < 1)].mean()\n",
    "            print(f\"  Average loan-to-income ratio: {avg_lti:.2f}\")\n",
    "    \n",
    "    # Summary of research questions and key findings\n",
    "    print(f\"\\\\n=== RESEARCH QUESTIONS SUMMARY ===\")\n",
    "    \n",
    "    key_findings = []\n",
    "    if 'grade' in df_loans.columns and 'int_rate' in df_loans.columns:\n",
    "        key_findings.append(\"Risk-based pricing shows clear grade differentiation with significant rate spreads\")\n",
    "    \n",
    "    if len(risk_factors) >= 2:\n",
    "        key_findings.append(\"Multiple risk factors (DTI, delinquencies, inquiries) correlate with pricing\")\n",
    "    \n",
    "    if 'annual_inc' in df_loans.columns and 'loan_amnt' in df_loans.columns:\n",
    "        key_findings.append(\"Borrower income profile significantly influences loan terms and approval amounts\")\n",
    "    \n",
    "    if date_columns:\n",
    "        key_findings.append(\"Temporal and geographic patterns reveal market dynamics and concentration\")\n",
    "    \n",
    "    print(\"Key findings for research question development:\")\n",
    "    for i, finding in enumerate(key_findings, 1):\n",
    "        print(f\"  {i}. {finding}\")\n",
    "    \n",
    "    print(f\"\\\\nResearch question specific visualizations completed\")\n",
    "    print(f\"Ready for hypothesis formulation and statistical testing\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset not available for research question visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlations(df, numeric_columns, method='pearson', threshold=0.3):\n",
    "    \"\"\"\n",
    "    Comprehensive correlation analysis with insights and visualization.\n",
    "    \n",
    "    This function calculates correlation matrices, identifies strong relationships,\n",
    "    and provides actionable insights for feature selection and research questions.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset for analysis\n",
    "    numeric_columns (list): List of numeric columns to analyze\n",
    "    method (str): Correlation method ('pearson', 'spearman', 'kendall')\n",
    "    threshold (float): Minimum correlation strength to report\n",
    "    \n",
    "    Returns:\n",
    "    dict: Correlation analysis results\n",
    "    \"\"\"\n",
    "    print(f\"Performing {method} correlation analysis on {len(numeric_columns)} variables...\")\n",
    "    \n",
    "    if len(numeric_columns) < 2:\n",
    "        print(\"Need at least 2 numeric variables for correlation analysis\")\n",
    "        return {}\n",
    "    \n",
    "    # Limit to available columns and first 15 for manageable analysis\n",
    "    available_cols = [col for col in numeric_columns if col in df.columns]\n",
    "    analysis_cols = available_cols[:15]\n",
    "    \n",
    "    print(f\"Analyzing correlations for {len(analysis_cols)} numeric variables\")\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_data = df[analysis_cols].select_dtypes(include=[np.number])\n",
    "    \n",
    "    if method == 'pearson':\n",
    "        corr_matrix = correlation_data.corr(method='pearson')\n",
    "    elif method == 'spearman':\n",
    "        corr_matrix = correlation_data.corr(method='spearman')\n",
    "    else:\n",
    "        corr_matrix = correlation_data.corr(method='kendall')\n",
    "    \n",
    "    # Find strong correlations (excluding self-correlations)\n",
    "    strong_correlations = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            var1 = corr_matrix.columns[i]\n",
    "            var2 = corr_matrix.columns[j]\n",
    "            corr_value = corr_matrix.iloc[i, j]\n",
    "            \n",
    "            if not pd.isna(corr_value) and abs(corr_value) >= threshold:\n",
    "                strong_correlations.append({\n",
    "                    'variable_1': var1,\n",
    "                    'variable_2': var2,\n",
    "                    'correlation': corr_value,\n",
    "                    'strength': 'Very Strong' if abs(corr_value) >= 0.8 else \n",
    "                               'Strong' if abs(corr_value) >= 0.6 else\n",
    "                               'Moderate' if abs(corr_value) >= 0.4 else 'Weak',\n",
    "                    'direction': 'Positive' if corr_value > 0 else 'Negative'\n",
    "                })\n",
    "    \n",
    "    # Sort by absolute correlation value\n",
    "    strong_correlations.sort(key=lambda x: abs(x['correlation']), reverse=True)\n",
    "    \n",
    "    # Display insights\n",
    "    print(f\"\\n=== CORRELATION ANALYSIS RESULTS ===\")\n",
    "    print(f\"Method: {method.capitalize()}\")\n",
    "    print(f\"Strong correlations found (|r| >= {threshold}): {len(strong_correlations)}\")\n",
    "    \n",
    "    if strong_correlations:\n",
    "        print(f\"\\nTop 10 strongest correlations:\")\n",
    "        for i, corr in enumerate(strong_correlations[:10]):\n",
    "            print(f\"{i+1:2d}. {corr['variable_1']} vs {corr['variable_2']}\")\n",
    "            print(f\"     Correlation: {corr['correlation']:.3f} ({corr['strength']}, {corr['direction']})\")\n",
    "    \n",
    "    # Identify multicollinearity concerns\n",
    "    high_corr = [corr for corr in strong_correlations if abs(corr['correlation']) >= 0.8]\n",
    "    if high_corr:\n",
    "        print(f\"\\nMulticollinearity concerns (|r| >= 0.8): {len(high_corr)}\")\n",
    "        for corr in high_corr:\n",
    "            print(f\"  - {corr['variable_1']} vs {corr['variable_2']}: {corr['correlation']:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'correlation_matrix': corr_matrix,\n",
    "        'strong_correlations': strong_correlations,\n",
    "        'analysis_columns': analysis_cols,\n",
    "        'method': method\n",
    "    }\n",
    "\n",
    "def create_correlation_heatmap(corr_matrix, title_suffix=''):\n",
    "    \"\"\"\n",
    "    Create publication-quality correlation heatmap with annotations.\n",
    "    \n",
    "    Parameters:\n",
    "    corr_matrix (DataFrame): Correlation matrix to visualize\n",
    "    title_suffix (str): Additional text for plot title\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays heatmap\n",
    "    \"\"\"\n",
    "    if corr_matrix.empty:\n",
    "        print(\"No correlation matrix provided for visualization\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Creating correlation heatmap for {corr_matrix.shape[0]} variables...\")\n",
    "    \n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create heatmap with custom styling\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Mask upper triangle\n",
    "    \n",
    "    heatmap = sns.heatmap(\n",
    "        corr_matrix,\n",
    "        mask=mask,\n",
    "        annot=True,\n",
    "        cmap='RdBu_r',\n",
    "        center=0,\n",
    "        fmt='.2f',\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": .8},\n",
    "        annot_kws={'size': 8}\n",
    "    )\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f'Correlation Matrix Heatmap {title_suffix}\\n(Lower Triangle Only)', \n",
    "             fontsize=14, fontweight='bold', pad =20)\n",
    "    plt.xlabel('Variables', fontsize=12)\n",
    "    plt.ylabel('Variables', fontsize=12)\n",
    "    \n",
    "    # Rotate labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Correlation heatmap visualization completed\")\n",
    "\n",
    "def identify_research_opportunities(strong_correlations, distribution_insights=None):\n",
    "    \"\"\"\n",
    "    Identify potential research questions based on correlation patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    strong_correlations (list): List of strong correlation relationships\n",
    "    distribution_insights (dict): Distribution analysis results\n",
    "    \n",
    "    Returns:\n",
    "    list: Potential research questions and hypotheses\n",
    "    \"\"\"\n",
    "    print(\"Identifying research opportunities from correlation patterns...\")\n",
    "    \n",
    "    research_opportunities = []\n",
    "    \n",
    "    if not strong_correlations:\n",
    "        print(\"No strong correlations found - limited research opportunities\")\n",
    "        return research_opportunities\n",
    "    \n",
    "    # Categorize correlations by strength and type\n",
    "    very_strong = [c for c in strong_correlations if abs(c['correlation']) >= 0.8]\n",
    "    strong_positive = [c for c in strong_correlations if c['correlation'] >= 0.6]\n",
    "    strong_negative = [c for c in strong_correlations if c['correlation'] <= -0.6]\n",
    "    \n",
    "    # Generate research questions based on patterns\n",
    "    if very_strong:\n",
    "        research_opportunities.append({\n",
    "            'category': 'Multicollinearity Investigation',\n",
    "            'question': f'Are {very_strong[0][\"variable_1\"]} and {very_strong[0][\"variable_2\"]} measuring similar underlying factors?',\n",
    "            'rationale': f'Very high correlation ({very_strong[0][\"correlation\"]:.3f}) suggests potential redundancy or shared causation',\n",
    "            'analysis_approach': 'Principal component analysis, factor analysis, or variable selection techniques'\n",
    "        })\n",
    "    \n",
    "    if strong_positive:\n",
    "        research_opportunities.append({\n",
    "            'category': 'Positive Relationship Analysis',\n",
    "            'question': f'How does {strong_positive[0][\"variable_1\"]} influence {strong_positive[0][\"variable_2\"]} in lending decisions?',\n",
    "            'rationale': f'Strong positive correlation ({strong_positive[0][\"correlation\"]:.3f}) suggests mutual reinforcement or causal relationship',\n",
    "            'analysis_approach': 'Regression analysis, causal inference, or predictive modeling'\n",
    "        })\n",
    "    \n",
    "    if strong_negative:\n",
    "        research_opportunities.append({\n",
    "            'category': 'Inverse Relationship Analysis', \n",
    "            'question': f'Why do {strong_negative[0][\"variable_1\"]} and {strong_negative[0][\"variable_2\"]} show opposing patterns?',\n",
    "            'rationale': f'Strong negative correlation ({strong_negative[0][\"correlation\"]:.3f}) indicates trade-off or substitution effects',\n",
    "            'analysis_approach': 'Risk-return analysis, segmentation analysis, or behavioral studies'\n",
    "        })\n",
    "    \n",
    "    # Add general research questions\n",
    "    if len(strong_correlations) >= 5:\n",
    "        research_opportunities.append({\n",
    "            'category': 'Predictive Modeling',\n",
    "            'question': 'Which combination of correlated variables best predicts loan outcomes?',\n",
    "            'rationale': f'Multiple strong correlations ({len(strong_correlations)}) provide rich feature set for prediction',\n",
    "            'analysis_approach': 'Machine learning models, feature selection, cross-validation'\n",
    "        })\n",
    "    \n",
    "    # Display opportunities\n",
    "    print(f\"\\n=== RESEARCH OPPORTUNITIES IDENTIFIED ===\")\n",
    "    print(f\"Total opportunities: {len(research_opportunities)}\")\n",
    "    \n",
    "    for i, opportunity in enumerate(research_opportunities):\n",
    "        print(f\"\\n{i+1}. {opportunity['category']}:\")\n",
    "        print(f\"   Question: {opportunity['question']}\")\n",
    "        print(f\"   Rationale: {opportunity['rationale']}\")\n",
    "        print(f\"   Approach: {opportunity['analysis_approach']}\")\n",
    "    \n",
    "    return research_opportunities\n",
    "\n",
    "print(\"Correlation analysis and research opportunity identification functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Correlation Analysis Visualizations Implementation\n",
    "if df_loans is not None and len(analysis_numeric_vars) >= 2:\n",
    "    print(\"=== PHASE 2: CORRELATION ANALYSIS VISUALIZATIONS ==\")\n",
    "    \n",
    "    # Perform enhanced correlation analysis with visualizations\n",
    "    correlation_results = analyze_correlations(\n",
    "        df_loans, \n",
    "        analysis_numeric_vars, \n",
    "        method='pearson', \n",
    "        threshold=0.3\n",
    "    )\n",
    "    \n",
    "    if 'correlation_matrix' in correlation_results:\n",
    "        corr_matrix = correlation_results['correlation_matrix']\n",
    "        \n",
    "        # 1. Enhanced correlation heatmap with better styling\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        # Create mask for upper triangle\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        \n",
    "        # Custom colormap\n",
    "        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "        \n",
    "        # Create heatmap\n",
    "        heatmap = sns.heatmap(\n",
    "            corr_matrix,\n",
    "            mask=mask,\n",
    "            annot=True,\n",
    "            cmap=cmap,\n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True,\n",
    "            linewidths=0.8,\n",
    "            cbar_kws={\"shrink\": .8, \"label\": \"Correlation Coefficient\"},\n",
    "            annot_kws={'size': 9, 'weight': 'bold'}\n",
    "        )\n",
    "        \n",
    "        plt.title('Phase 2: Pearson Correlation Matrix\\\\n(Lower Triangle Only)', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('Variables', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Variables', fontsize=12, fontweight='bold')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Correlation strength distribution\n",
    "        # Get all correlation values (excluding self-correlations)\n",
    "        corr_values = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if not pd.isna(corr_val):\n",
    "                    corr_values.append(abs(corr_val))\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Histogram of correlation strengths\n",
    "        ax1.hist(corr_values, bins=20, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "        ax1.axvline(np.mean(corr_values), color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean: {np.mean(corr_values):.3f}')\n",
    "        ax1.axvline(0.3, color='orange', linestyle='--', linewidth=2, \n",
    "                   label='Threshold: 0.3')\n",
    "        ax1.set_title('Distribution of Absolute Correlation Coefficients', fontweight='bold')\n",
    "        ax1.set_xlabel('Absolute Correlation')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Correlation strength categories\n",
    "        categories = ['Weak\\\\n(0-0.3)', 'Moderate\\\\n(0.3-0.6)', 'Strong\\\\n(0.6-0.8)', 'Very Strong\\\\n(0.8-1.0)']\n",
    "        counts = [\n",
    "            sum(1 for x in corr_values if x < 0.3),\n",
    "            sum(1 for x in corr_values if 0.3 <= x < 0.6),\n",
    "            sum(1 for x in corr_values if 0.6 <= x < 0.8),\n",
    "            sum(1 for x in corr_values if x >= 0.8)\n",
    "        ]\n",
    "        \n",
    "        colors = ['lightgray', 'lightblue', 'orange', 'red']\n",
    "        bars = ax2.bar(categories, counts, color=colors, alpha=0.7, edgecolor='navy')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax2.set_title('Correlation Strength Categories', fontweight='bold')\n",
    "        ax2.set_ylabel('Number of Variable Pairs')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.suptitle('Phase 2: Correlation Analysis Summary', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Top correlations scatter plots\n",
    "        strong_corrs = correlation_results.get('strong_correlations', [])\n",
    "        if strong_corrs:\n",
    "            print(f\"\\\\n=== TOP CORRELATION RELATIONSHIPS ===\")\n",
    "            \n",
    "            # Select top 6 correlations for scatter plots\n",
    "            top_corrs = strong_corrs[:6]\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i, corr_info in enumerate(top_corrs):\n",
    "                var1 = corr_info['variable_1']\n",
    "                var2 = corr_info['variable_2']\n",
    "                corr_val = corr_info['correlation']\n",
    "                \n",
    "                if var1 in df_loans.columns and var2 in df_loans.columns:\n",
    "                    # Sample data for plotting (to avoid overplotting)\n",
    "                    sample_data = df_loans[[var1, var2]].dropna().sample(\n",
    "                        min(2000, len(df_loans)), random_state=42)\n",
    "                    \n",
    "                    # Create scatter plot\n",
    "                    axes[i].scatter(sample_data[var1], sample_data[var2], \n",
    "                                  alpha=0.5, s=20, c='lightblue', edgecolors='navy')\n",
    "                    \n",
    "                    # Add trend line\n",
    "                    z = np.polyfit(sample_data[var1], sample_data[var2], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    x_trend = np.linspace(sample_data[var1].min(), sample_data[var1].max(), 100)\n",
    "                    axes[i].plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2)\n",
    "                    \n",
    "                    # Formatting\n",
    "                    axes[i].set_title(f'{var1} vs {var2}\\\\nCorr: {corr_val:.3f} ({corr_info[\"strength\"]})', \n",
    "                                    fontsize=11, fontweight='bold')\n",
    "                    axes[i].set_xlabel(var1)\n",
    "                    axes[i].set_ylabel(var2)\n",
    "                    axes[i].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    print(f\"  {i+1}. {var1} â {var2}: {corr_val:.3f} ({corr_info['direction']} {corr_info['strength']})\")\n",
    "            \n",
    "            # Hide empty subplots\n",
    "            for i in range(len(top_corrs), len(axes)):\n",
    "                axes[i].set_visible(False)\n",
    "            \n",
    "            plt.suptitle('Phase 2: Top Correlation Relationships - Scatter Plots', \n",
    "                        fontsize=16, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # 4. Spearman vs Pearson comparison\n",
    "        print(f\"\\\\n=== SPEARMAN VS PEARSON COMPARISON ===\")\n",
    "        spearman_results = analyze_correlations(\n",
    "            df_loans, \n",
    "            analysis_numeric_vars[:10], \n",
    "            method='spearman', \n",
    "            threshold=0.3\n",
    "        )\n",
    "        \n",
    "        if spearman_results:\n",
    "            pearson_count = len(strong_corrs)\n",
    "            spearman_count = len(spearman_results.get('strong_correlations', []))\n",
    "            \n",
    "            comparison_data = ['Pearson', 'Spearman']\n",
    "            comparison_counts = [pearson_count, spearman_count]\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            bars = plt.bar(comparison_data, comparison_counts, \n",
    "                          color=['lightblue', 'lightcoral'], alpha=0.7, edgecolor='navy')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, count in zip(bars, comparison_counts):\n",
    "                height = bar.get_height()\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                        f'{count}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            plt.title('Correlation Methods Comparison\\\\n(Strong Correlations Found)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "            plt.ylabel('Number of Strong Correlations (|r| â¥ 0.3)')\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add interpretation\n",
    "            if spearman_count > pearson_count:\n",
    "                plt.text(0.5, max(comparison_counts) * 0.8, \n",
    "                        f'Non-linear relationships detected\\\\n({spearman_count - pearson_count} additional)',\n",
    "                        ha='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "            elif pearson_count > spearman_count:\n",
    "                plt.text(0.5, max(comparison_counts) * 0.8,\n",
    "                        'Linear relationships dominate',\n",
    "                        ha='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7))\n",
    "            else:\n",
    "                plt.text(0.5, max(comparison_counts) * 0.8,\n",
    "                        'Similar linear/non-linear patterns',\n",
    "                        ha='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.7))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"  Pearson correlations (linear): {pearson_count}\")\n",
    "            print(f\"  Spearman correlations (monotonic): {spearman_count}\")\n",
    "            print(f\"  Difference: {abs(spearman_count - pearson_count)} additional relationships\")\n",
    "        \n",
    "        print(f\"\\\\nPhase 2 completed: Correlation analysis with {len(corr_matrix.columns)} variables\")\n",
    "    else:\n",
    "        print(\"Correlation matrix not available for Phase 2 visualization\")\n",
    "else:\n",
    "    print(\"Insufficient numeric variables for Phase 2 correlation analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional correlation analysis with Spearman method for comparison\n",
    "if df_loans is not None and len(analysis_numeric_vars) >= 2:\n",
    "    print(\"=== SPEARMAN CORRELATION COMPARISON ===\")\n",
    "    \n",
    "    # Spearman correlation for non-linear relationships\n",
    "    spearman_results = analyze_correlations(\n",
    "        df_loans, \n",
    "        analysis_numeric_vars[:10], \n",
    "        method='spearman', \n",
    "        threshold=0.3\n",
    "    )\n",
    "    \n",
    "    # Compare Pearson vs Spearman results\n",
    "    if 'correlation_results' in locals() and spearman_results:\n",
    "        pearson_strong = len(correlation_results.get('strong_correlations', []))\n",
    "        spearman_strong = len(spearman_results.get('strong_correlations', []))\n",
    "        \n",
    "        print(f\"\\nCorrelation Method Comparison:\")\n",
    "        print(f\"  Pearson strong correlations: {pearson_strong}\")\n",
    "        print(f\"  Spearman strong correlations: {spearman_strong}\")\n",
    "        \n",
    "        if spearman_strong > pearson_strong:\n",
    "            print(f\"  Insight: More non-linear relationships detected ({spearman_strong - pearson_strong} additional)\")\n",
    "        elif pearson_strong > spearman_strong:\n",
    "            print(f\"  Insight: Primarily linear relationships dominate\")\n",
    "        else:\n",
    "            print(f\"  Insight: Similar linear and non-linear relationship patterns\")\n",
    "else:\n",
    "    print(\"Skipping Spearman correlation comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive EDA Summary and Insights\n",
    "\n",
    "Final synthesis of exploratory data analysis findings with actionable insights for research question development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eda_summary(df, structure_analysis, stats_results, correlation_results, distribution_results):\n",
    "    \"\"\"\n",
    "    Generate comprehensive EDA summary with key insights and recommendations.\n",
    "    \n",
    "    This function synthesizes all analysis results into actionable insights\n",
    "    for research question development and further analysis directions.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Original dataset\n",
    "    structure_analysis (dict): Dataset structure analysis results\n",
    "    stats_results (dict): Statistical summary results\n",
    "    correlation_results (dict): Correlation analysis results\n",
    "    distribution_results (dict): Distribution analysis results\n",
    "    \n",
    "    Returns:\n",
    "    dict: Comprehensive summary with insights\n",
    "    \"\"\"\n",
    "    print(\"Generating comprehensive EDA summary and insights...\")\n",
    "    \n",
    "    summary = {\n",
    "        'dataset_overview': {},\n",
    "        'data_quality_assessment': {},\n",
    "        'key_findings': [],\n",
    "        'research_recommendations': [],\n",
    "        'analysis_readiness': {}\n",
    "    }\n",
    "    \n",
    "    # Dataset Overview\n",
    "    if df is not None:\n",
    "        summary['dataset_overview'] = {\n",
    "            'total_records': len(df),\n",
    "            'total_variables': len(df.columns),\n",
    "            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "            'analysis_period': 'Sample data for development'\n",
    "        }\n",
    "    \n",
    "    # Data Quality Assessment\n",
    "    if structure_analysis:\n",
    "        quality_summary = structure_analysis.get('quality_summary', {})\n",
    "        dataset_info = structure_analysis.get('dataset_info', {})\n",
    "        \n",
    "        total_missing_pct = (dataset_info.get('total_missing', 0) / \n",
    "                            (summary['dataset_overview']['total_records'] * \n",
    "                             summary['dataset_overview']['total_variables'])) * 100\n",
    "        \n",
    "        summary['data_quality_assessment'] = {\n",
    "            'overall_completeness': 100 - total_missing_pct,\n",
    "            'high_quality_variables': quality_summary.get('high_quality_cols', 0),\n",
    "            'medium_quality_variables': quality_summary.get('medium_quality_cols', 0),\n",
    "            'low_quality_variables': quality_summary.get('low_quality_cols', 0),\n",
    "            'numeric_variables': quality_summary.get('numeric_cols', 0),\n",
    "            'categorical_variables': quality_summary.get('categorical_cols', 0),\n",
    "            'data_quality_score': 'Excellent' if total_missing_pct < 5 else \n",
    "                                 'Good' if total_missing_pct < 15 else \n",
    "                                 'Fair' if total_missing_pct < 30 else 'Poor'\n",
    "        }\n",
    "    \n",
    "    # Key Statistical Findings\n",
    "    if stats_results:\n",
    "        numeric_stats = stats_results.get('numeric_summary', {})\n",
    "        if numeric_stats:\n",
    "            # Identify highly skewed variables\n",
    "            highly_skewed = [(var, stats['skewness']) for var, stats in numeric_stats.items() \n",
    "                           if abs(stats['skewness']) > 2]\n",
    "            \n",
    "            # Identify high variability variables\n",
    "            high_variability = [(var, stats['cv']) for var, stats in numeric_stats.items() \n",
    "                              if stats['cv'] > 1]\n",
    "            \n",
    "            summary['key_findings'].extend([\n",
    "                f\"Identified {len(highly_skewed)} highly skewed variables requiring transformation\",\n",
    "                f\"Found {len(high_variability)} variables with high coefficient of variation (>1.0)\",\n",
    "                f\"Statistical analysis completed for {len(numeric_stats)} numeric variables\"\n",
    "            ])\n",
    "    \n",
    "    # Correlation Findings\n",
    "    if correlation_results:\n",
    "        strong_corrs = correlation_results.get('strong_correlations', [])\n",
    "        if strong_corrs:\n",
    "            very_strong = [c for c in strong_corrs if abs(c['correlation']) >= 0.8]\n",
    "            moderate_strong = [c for c in strong_corrs if 0.4 <= abs(c['correlation']) < 0.8]\n",
    "            \n",
    "            summary['key_findings'].extend([\n",
    "                f\"Discovered {len(strong_corrs)} significant variable relationships (|r| >= 0.3)\",\n",
    "                f\"Identified {len(very_strong)} potential multicollinearity concerns (|r| >= 0.8)\",\n",
    "                f\"Found {len(moderate_strong)} moderate to strong correlations for analysis\"\n",
    "            ])\n",
    "    \n",
    "    # Distribution Findings\n",
    "    if distribution_results:\n",
    "        high_quality_distributions = sum(1 for insights in distribution_results.values() \n",
    "                                        if insights.get('data_quality') == 'High')\n",
    "        outlier_heavy = sum(1 for insights in distribution_results.values() \n",
    "                           if insights.get('outlier_percentage', 0) > 10)\n",
    "        \n",
    "        summary['key_findings'].extend([\n",
    "            f\"Analyzed {len(distribution_results)} variable distributions\",\n",
    "            f\"{high_quality_distributions} variables show high-quality distributions\",\n",
    "            f\"{outlier_heavy} variables require outlier treatment (>10% outliers)\"\n",
    "        ])\n",
    "    \n",
    "    # Research Recommendations\n",
    "    summary['research_recommendations'] = [\n",
    "        \"Focus on high-quality variables with strong correlations for predictive modeling\",\n",
    "        \"Investigate causal relationships between strongly correlated variables\",\n",
    "        \"Apply appropriate transformations to highly skewed variables\",\n",
    "        \"Consider dimensionality reduction for variables with multicollinearity\",\n",
    "        \"Develop hypotheses based on correlation patterns and business logic\"\n",
    "    ]\n",
    "    \n",
    "    # Analysis Readiness Assessment\n",
    "    readiness_score = 0\n",
    "    if summary['data_quality_assessment'].get('overall_completeness', 0) > 90:\n",
    "        readiness_score += 25\n",
    "    if correlation_results and len(correlation_results.get('strong_correlations', [])) > 5:\n",
    "        readiness_score += 25\n",
    "    if stats_results and len(stats_results.get('numeric_summary', {})) > 5:\n",
    "        readiness_score += 25\n",
    "    if structure_analysis and structure_analysis.get('quality_summary', {}).get('high_quality_cols', 0) > 10:\n",
    "        readiness_score += 25\n",
    "    \n",
    "    summary['analysis_readiness'] = {\n",
    "        'readiness_score': readiness_score,\n",
    "        'readiness_level': 'Excellent' if readiness_score >= 90 else \n",
    "                          'Good' if readiness_score >= 70 else\n",
    "                          'Fair' if readiness_score >= 50 else 'Poor',\n",
    "        'ready_for_modeling': readiness_score >= 70,\n",
    "        'recommended_next_steps': [\n",
    "            'Feature engineering and selection',\n",
    "            'Research question formulation', \n",
    "            'Hypothesis development',\n",
    "            'Predictive model development'\n",
    "        ] if readiness_score >= 70 else [\n",
    "            'Additional data cleaning required',\n",
    "            'Missing value imputation needed',\n",
    "            'Data quality improvement necessary'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(\"EDA summary generation function defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive EDA summary\n",
    "if df_loans is not None:\n",
    "    print(\"=== COMPREHENSIVE EDA SUMMARY ===\")\n",
    "    \n",
    "    # Collect all analysis results\n",
    "    eda_summary = generate_eda_summary(\n",
    "        df_loans,\n",
    "        structure_analysis if 'structure_analysis' in locals() else {},\n",
    "        stats_results if 'stats_results' in locals() else {},\n",
    "        correlation_results if 'correlation_results' in locals() else {},\n",
    "        distribution_results if 'distribution_results' in locals() else {}\n",
    "    )\n",
    "    \n",
    "    # Display summary results\n",
    "    print(f\"\\n=== DATASET OVERVIEW ===\")\n",
    "    overview = eda_summary['dataset_overview']\n",
    "    print(f\"Total records: {overview.get('total_records', 'N/A'):,}\")\n",
    "    print(f\"Total variables: {overview.get('total_variables', 'N/A'):,}\")\n",
    "    print(f\"Memory usage: {overview.get('memory_usage_mb', 0):.2f} MB\")\n",
    "    \n",
    "    print(f\"\\n=== DATA QUALITY ASSESSMENT ===\")\n",
    "    quality = eda_summary['data_quality_assessment']\n",
    "    print(f\"Overall completeness: {quality.get('overall_completeness', 0):.1f}%\")\n",
    "    print(f\"Data quality score: {quality.get('data_quality_score', 'Unknown')}\")\n",
    "    print(f\"High quality variables: {quality.get('high_quality_variables', 0)}\")\n",
    "    print(f\"Numeric variables: {quality.get('numeric_variables', 0)}\")\n",
    "    print(f\"Categorical variables: {quality.get('categorical_variables', 0)}\")\n",
    "    \n",
    "    print(f\"\\n=== KEY FINDINGS ===\")\n",
    "    for i, finding in enumerate(eda_summary['key_findings'], 1):\n",
    "        print(f\"{i:2d}. {finding}\")\n",
    "    \n",
    "    print(f\"\\n=== RESEARCH RECOMMENDATIONS ===\")\n",
    "    for i, rec in enumerate(eda_summary['research_recommendations'], 1):\n",
    "        print(f\"{i:2d}. {rec}\")\n",
    "    \n",
    "    print(f\"\\n=== ANALYSIS READINESS ===\")\n",
    "    readiness = eda_summary['analysis_readiness']\n",
    "    print(f\"Readiness score: {readiness.get('readiness_score', 0)}/100\")\n",
    "    print(f\"Readiness level: {readiness.get('readiness_level', 'Unknown')}\")\n",
    "    print(f\"Ready for modeling: {'Yes' if readiness.get('ready_for_modeling', False) else 'No'}\")\n",
    "    \n",
    "    print(f\"\\nRecommended next steps:\")\n",
    "    for i, step in enumerate(readiness.get('recommended_next_steps', []), 1):\n",
    "        print(f\"  {i}. {step}\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"EXPLORATORY DATA ANALYSIS COMPLETED SUCCESSFULLY\")\n",
    "    print(f\"Dataset is ready for feature engineering and research question development\")\n",
    "    print(f\"{'='*50}\")\n",
    "else:\n",
    "    print(\"No dataset available for EDA summary generation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
