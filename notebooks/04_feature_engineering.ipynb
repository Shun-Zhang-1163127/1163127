{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Encoding Implementation\n",
    "# Based on LAB4 materials - efficient for high cardinality categorical features\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "def apply_binary_encoding(df, categorical_columns):\n",
    "    \"\"\"\n",
    "    Apply binary encoding for high cardinality categorical features\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with categorical features\n",
    "    categorical_columns: list of column names to encode\n",
    "    \n",
    "    Returns:\n",
    "    df_encoded: DataFrame with binary encoded features\n",
    "    encoder: fitted binary encoder for inverse transform\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    print(\"Applying Binary Encoding...\")\n",
    "    print(\"Binary encoding reduces dimensionality compared to one-hot encoding\")\n",
    "    print(\"Example: 8 categories need only 3 binary columns (2^3 = 8)\")\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in df_encoded.columns:\n",
    "            # Handle missing values\n",
    "            df_encoded[col] = df_encoded[col].fillna('Unknown')\n",
    "            \n",
    "            # Apply binary encoding\n",
    "            encoder = ce.BinaryEncoder(cols=[col])\n",
    "            encoded_cols = encoder.fit_transform(df_encoded[[col]])\n",
    "            \n",
    "            # Add encoded columns to dataframe\n",
    "            for encoded_col in encoded_cols.columns:\n",
    "                df_encoded[f\"{col}_bin_{encoded_col}\"] = encoded_cols[encoded_col]\n",
    "            \n",
    "            n_categories = df_encoded[col].nunique()\n",
    "            n_binary_cols = len(encoded_cols.columns)\n",
    "            print(f\"  {col}: {n_categories} categories -> {n_binary_cols} binary columns\")\n",
    "            \n",
    "            # Show the binary encoding pattern\n",
    "            if n_categories <= 8:  # Only show for small examples\n",
    "                unique_vals = df_encoded[col].unique()[:8]  # Show first 8\n",
    "                print(f\"    Encoding pattern for {col}:\")\n",
    "                for val in unique_vals:\n",
    "                    mask = df_encoded[col] == val\n",
    "                    if mask.any():\n",
    "                        binary_vals = encoded_cols[mask].iloc[0].values\n",
    "                        print(f\"      '{val}' -> {binary_vals}\")\n",
    "    \n",
    "    return df_encoded, encoder\n",
    "\n",
    "# Example usage with high cardinality categorical features\n",
    "if not df.empty and categorical_features:\n",
    "    # Find features with high cardinality (good candidates for binary encoding)\n",
    "    high_cardinality_features = []\n",
    "    for col in categorical_features:\n",
    "        if col in df.columns:\n",
    "            n_unique = df[col].nunique()\n",
    "            if 5 < n_unique <= 50:  # Good range for binary encoding\n",
    "                high_cardinality_features.append(col)\n",
    "    \n",
    "    if high_cardinality_features:\n",
    "        # Take first feature for demonstration\n",
    "        demo_feature = high_cardinality_features[0]\n",
    "        print(f\"Demonstrating Binary Encoding on: {demo_feature}\")\n",
    "        print(f\"  Unique values: {df[demo_feature].nunique()}\")\n",
    "        \n",
    "        df_binary_encoded, binary_encoder = apply_binary_encoding(df, [demo_feature])\n",
    "        \n",
    "        # Show comparison of dimensions\n",
    "        binary_cols = [c for c in df_binary_encoded.columns if c.startswith(demo_feature + '_bin_')]\n",
    "        print(f\"\\nDimensionality comparison:\")\n",
    "        print(f\"  One-hot would create: {df[demo_feature].nunique()} columns\")  \n",
    "        print(f\"  Binary encoding creates: {len(binary_cols)} columns\")\n",
    "        print(f\"  Space savings: {((df[demo_feature].nunique() - len(binary_cols)) / df[demo_feature].nunique() * 100):.1f}%\")\n",
    "    else:\n",
    "        print(\"No suitable high cardinality features found for Binary Encoding demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Feature Engineering Pipeline\n",
    "# Integrates all feature engineering techniques in a configurable pipeline\n",
    "\n",
    "def feature_engineering_pipeline(df, config=None):\n",
    "    \"\"\"\n",
    "    Complete feature engineering pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    df: input DataFrame\n",
    "    config: configuration dict with encoding/scaling preferences\n",
    "    \n",
    "    Returns:\n",
    "    df_processed: fully processed DataFrame\n",
    "    pipeline_info: information about transformations applied\n",
    "    encoders: dict of fitted encoders for inverse transforms\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        # Default configuration\n",
    "        config = {\n",
    "            'categorical_encoding': 'auto',  # 'auto', 'label', 'onehot', 'binary'\n",
    "            'numerical_scaling': 'standard',  # 'standard', 'minmax', 'none'\n",
    "            'handle_missing': True,\n",
    "            'apply_smote': False,\n",
    "            'target_column': None\n",
    "        }\n",
    "    \n",
    "    print(\"=== Feature Engineering Pipeline ===\")\n",
    "    print(f\"Input data shape: {df.shape}\")\n",
    "    \n",
    "    # Initialize result dataframe and info\n",
    "    df_processed = df.copy()\n",
    "    pipeline_info = {\n",
    "        'original_shape': df.shape,\n",
    "        'categorical_features': [],\n",
    "        'numerical_features': [],\n",
    "        'transformations_applied': [],\n",
    "        'final_shape': None\n",
    "    }\n",
    "    encoders = {}\n",
    "    \n",
    "    # Identify feature types\n",
    "    categorical_features = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_features = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove target column from features if specified\n",
    "    if config['target_column'] and config['target_column'] in numerical_features:\n",
    "        numerical_features.remove(config['target_column'])\n",
    "    if config['target_column'] and config['target_column'] in categorical_features:\n",
    "        categorical_features.remove(config['target_column'])\n",
    "    \n",
    "    pipeline_info['categorical_features'] = categorical_features\n",
    "    pipeline_info['numerical_features'] = numerical_features\n",
    "    \n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"Numerical features: {len(numerical_features)}\")\n",
    "    \n",
    "    # Step 1: Handle missing values\n",
    "    if config['handle_missing']:\n",
    "        print(\"\\nStep 1: Handling missing values...\")\n",
    "        # Fill categorical missing values\n",
    "        for col in categorical_features:\n",
    "            if df_processed[col].isnull().any():\n",
    "                df_processed[col] = df_processed[col].fillna('Unknown')\n",
    "        \n",
    "        # Fill numerical missing values with mean\n",
    "        for col in numerical_features:\n",
    "            if df_processed[col].isnull().any():\n",
    "                df_processed[col] = df_processed[col].fillna(df_processed[col].mean())\n",
    "        \n",
    "        pipeline_info['transformations_applied'].append('missing_value_imputation')\n",
    "    \n",
    "    # Step 2: Categorical encoding\n",
    "    if categorical_features:\n",
    "        print(f\"\\nStep 2: Categorical encoding ({config['categorical_encoding']})...\")\n",
    "        \n",
    "        if config['categorical_encoding'] == 'auto':\n",
    "            # Automatically choose encoding based on cardinality\n",
    "            for col in categorical_features:\n",
    "                n_unique = df_processed[col].nunique()\n",
    "                if n_unique == 2:\n",
    "                    # Binary features - use label encoding\n",
    "                    df_processed, col_encoders = apply_label_encoding(df_processed, [col])\n",
    "                    encoders.update(col_encoders)\n",
    "                elif n_unique <= 10:\n",
    "                    # Low cardinality - use one-hot encoding\n",
    "                    df_processed = apply_onehot_encoding(df_processed, [col])\n",
    "                else:\n",
    "                    # High cardinality - use binary encoding\n",
    "                    df_processed, col_encoder = apply_binary_encoding(df_processed, [col])\n",
    "                    encoders[col] = col_encoder\n",
    "        \n",
    "        elif config['categorical_encoding'] == 'label':\n",
    "            df_processed, label_encoders = apply_label_encoding(df_processed, categorical_features)\n",
    "            encoders.update(label_encoders)\n",
    "        \n",
    "        elif config['categorical_encoding'] == 'onehot':\n",
    "            df_processed = apply_onehot_encoding(df_processed, categorical_features)\n",
    "        \n",
    "        elif config['categorical_encoding'] == 'binary':\n",
    "            for col in categorical_features:\n",
    "                df_processed, col_encoder = apply_binary_encoding(df_processed, [col])\n",
    "                encoders[col] = col_encoder\n",
    "        \n",
    "        pipeline_info['transformations_applied'].append(f\"categorical_encoding_{config['categorical_encoding']}\")\n",
    "    \n",
    "    # Step 3: Numerical scaling\n",
    "    if numerical_features and config['numerical_scaling'] != 'none':\n",
    "        print(f\"\\nStep 3: Numerical scaling ({config['numerical_scaling']})...\")\n",
    "        \n",
    "        if config['numerical_scaling'] == 'standard':\n",
    "            df_processed, scaler = apply_standard_scaling(df_processed, numerical_features)\n",
    "            encoders['numerical_scaler'] = scaler\n",
    "        \n",
    "        elif config['numerical_scaling'] == 'minmax':\n",
    "            df_processed, scaler = apply_minmax_scaling(df_processed, numerical_features)\n",
    "            encoders['numerical_scaler'] = scaler\n",
    "        \n",
    "        pipeline_info['transformations_applied'].append(f\"numerical_scaling_{config['numerical_scaling']}\")\n",
    "    \n",
    "    # Update final shape\n",
    "    pipeline_info['final_shape'] = df_processed.shape\n",
    "    \n",
    "    print(f\"\\n=== Pipeline Complete ===\")\n",
    "    print(f\"Final data shape: {df_processed.shape}\")\n",
    "    print(f\"Transformations applied: {', '.join(pipeline_info['transformations_applied'])}\")\n",
    "    \n",
    "    return df_processed, pipeline_info, encoders\n",
    "\n",
    "# Example usage of complete pipeline\n",
    "if not df.empty:\n",
    "    print(\"Demonstrating Complete Feature Engineering Pipeline\")\n",
    "    \n",
    "    # Define pipeline configuration\n",
    "    pipeline_config = {\n",
    "        'categorical_encoding': 'auto',\n",
    "        'numerical_scaling': 'standard',\n",
    "        'handle_missing': True,\n",
    "        'apply_smote': False,\n",
    "        'target_column': 'loan_status' if 'loan_status' in df.columns else None\n",
    "    }\n",
    "    \n",
    "    # Apply complete pipeline\n",
    "    df_final, info, all_encoders = feature_engineering_pipeline(df, pipeline_config)\n",
    "    \n",
    "    # Show pipeline results\n",
    "    print(f\"\\nPipeline Results:\")\n",
    "    print(f\"Original features: {info['original_shape'][1]}\")\n",
    "    print(f\"Final features: {info['final_shape'][1]}\")\n",
    "    print(f\"Feature expansion: {info['final_shape'][1] - info['original_shape'][1]} new features\")\n",
    "    print(f\"Sample size: {info['final_shape'][0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE Implementation for Class Imbalance\n",
    "# Based on LAB4 materials - handles imbalanced classification datasets\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "def handle_class_imbalance(X, y, method='smote', random_state=42):\n",
    "    \"\"\"\n",
    "    Handle class imbalance using SMOTE technique\n",
    "    \n",
    "    Parameters:\n",
    "    X: feature matrix (numerical features only)\n",
    "    y: target variable \n",
    "    method: resampling method ('smote', 'adasyn', 'borderline')\n",
    "    random_state: random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    X_resampled: resampled feature matrix\n",
    "    y_resampled: resampled target variable\n",
    "    \"\"\"\n",
    "    print(\"Handling Class Imbalance...\")\n",
    "    print(f\"Original class distribution:\")\n",
    "    original_counts = Counter(y)\n",
    "    for class_label, count in original_counts.items():\n",
    "        print(f\"  Class {class_label}: {count} samples ({count/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    if method.lower() == 'smote':\n",
    "        # Standard SMOTE\n",
    "        smote = SMOTE(random_state=random_state)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    else:\n",
    "        print(f\"Method '{method}' not implemented, using SMOTE\")\n",
    "        smote = SMOTE(random_state=random_state)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    print(f\"\\nAfter SMOTE resampling:\")\n",
    "    resampled_counts = Counter(y_resampled)\n",
    "    for class_label, count in resampled_counts.items():\n",
    "        print(f\"  Class {class_label}: {count} samples ({count/len(y_resampled)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nDataset size change: {len(y)} -> {len(y_resampled)} samples\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Example usage for class imbalance handling\n",
    "if not df.empty and numerical_features:\n",
    "    # Check if we have a target variable for classification\n",
    "    target_column = None\n",
    "    possible_targets = ['loan_status', 'default', 'target']\n",
    "    \n",
    "    for col in possible_targets:\n",
    "        if col in df.columns:\n",
    "            target_column = col\n",
    "            break\n",
    "    \n",
    "    if target_column and len(numerical_features) >= 5:\n",
    "        # Prepare numerical features for SMOTE\n",
    "        sample_features = numerical_features[:5]  # Use first 5 numerical features\n",
    "        X_sample = df[sample_features].fillna(df[sample_features].mean())\n",
    "        \n",
    "        # Create binary target if needed\n",
    "        if target_column in df.columns:\n",
    "            if df[target_column].nunique() == 2:\n",
    "                y_sample = df[target_column]\n",
    "            else:\n",
    "                # Convert to binary (e.g., good vs bad loans)\n",
    "                y_sample = (df[target_column] == df[target_column].mode()[0]).astype(int)\n",
    "            \n",
    "            print(f\"Demonstrating SMOTE on {len(sample_features)} features with target '{target_column}'\")\n",
    "            \n",
    "            # Apply SMOTE\n",
    "            X_resampled, y_resampled = handle_class_imbalance(X_sample, y_sample)\n",
    "            \n",
    "            # Visualize class distribution\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            # Original distribution\n",
    "            original_counts = Counter(y_sample)\n",
    "            classes = list(original_counts.keys())\n",
    "            counts = list(original_counts.values())\n",
    "            axes[0].bar(classes, counts, alpha=0.7, color='blue')\n",
    "            axes[0].set_title('Original Class Distribution')\n",
    "            axes[0].set_xlabel('Class')\n",
    "            axes[0].set_ylabel('Count')\n",
    "            \n",
    "            # After SMOTE\n",
    "            resampled_counts = Counter(y_resampled)\n",
    "            classes_resampled = list(resampled_counts.keys())\n",
    "            counts_resampled = list(resampled_counts.values())\n",
    "            axes[1].bar(classes_resampled, counts_resampled, alpha=0.7, color='red')\n",
    "            axes[1].set_title('After SMOTE Resampling')\n",
    "            axes[1].set_xlabel('Class')\n",
    "            axes[1].set_ylabel('Count')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Target column '{target_column}' not found in dataframe\")\n",
    "    else:\n",
    "        print(\"SMOTE demonstration requires a target variable and numerical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling Implementation\n",
    "# Based on LAB4 materials - scales features to range [0, 1]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def apply_minmax_scaling(df, numerical_columns, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Apply Min-Max scaling to numerical features\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with numerical features\n",
    "    numerical_columns: list of column names to scale\n",
    "    feature_range: tuple defining the target range (default: (0, 1))\n",
    "    \n",
    "    Returns:\n",
    "    df_scaled: DataFrame with scaled features\n",
    "    scaler: fitted MinMaxScaler for inverse transform\n",
    "    \"\"\"\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    print(\"Applying Min-Max Scaling...\")\n",
    "    print(f\"Scaling numerical features to range {feature_range}\")\n",
    "    \n",
    "    if numerical_columns:\n",
    "        # Initialize scaler\n",
    "        scaler = MinMaxScaler(feature_range=feature_range)\n",
    "        \n",
    "        # Select only numerical columns that exist in dataframe\n",
    "        valid_columns = [col for col in numerical_columns if col in df_scaled.columns]\n",
    "        \n",
    "        if valid_columns:\n",
    "            # Apply scaling\n",
    "            df_scaled[valid_columns] = scaler.fit_transform(df_scaled[valid_columns])\n",
    "            \n",
    "            print(f\"Scaled {len(valid_columns)} numerical features:\")\n",
    "            for col in valid_columns[:5]:  # Show first 5 features\n",
    "                original_min = df[col].min()\n",
    "                original_max = df[col].max()\n",
    "                scaled_min = df_scaled[col].min()\n",
    "                scaled_max = df_scaled[col].max()\n",
    "                print(f\"  {col}: [{original_min:.2f}, {original_max:.2f}] -> [{scaled_min:.2f}, {scaled_max:.2f}]\")\n",
    "            \n",
    "            if len(valid_columns) > 5:\n",
    "                print(f\"  ... and {len(valid_columns) - 5} more features\")\n",
    "        else:\n",
    "            print(\"No valid numerical columns found for scaling\")\n",
    "            scaler = None\n",
    "    else:\n",
    "        print(\"No numerical columns provided for scaling\")\n",
    "        scaler = None\n",
    "    \n",
    "    return df_scaled, scaler\n",
    "\n",
    "# Example usage with sample numerical features\n",
    "if not df.empty and numerical_features:\n",
    "    # Select a subset of numerical features for demonstration\n",
    "    # Exclude ID columns and binary features\n",
    "    sample_numerical = []\n",
    "    for col in numerical_features[:10]:  # Take first 10 numerical features\n",
    "        if col in df.columns and not col.lower().endswith('_id'):\n",
    "            # Check if feature has reasonable range (not binary 0/1)\n",
    "            if df[col].nunique() > 2:\n",
    "                sample_numerical.append(col)\n",
    "    \n",
    "    if sample_numerical:\n",
    "        print(f\"Demonstrating Min-Max Scaling on: {len(sample_numerical)} features\")\n",
    "        df_minmax_scaled, minmax_scaler = apply_minmax_scaling(df, sample_numerical)\n",
    "        \n",
    "        # Show comparison of original vs scaled distributions\n",
    "        if len(sample_numerical) >= 2:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "            \n",
    "            # Plot first two features for comparison\n",
    "            for i, col in enumerate(sample_numerical[:2]):\n",
    "                # Original distribution\n",
    "                axes[i, 0].hist(df[col].dropna(), bins=30, alpha=0.7, color='blue')\n",
    "                axes[i, 0].set_title(f'Original: {col}')\n",
    "                axes[i, 0].set_xlabel('Value')\n",
    "                axes[i, 0].set_ylabel('Frequency')\n",
    "                \n",
    "                # Scaled distribution\n",
    "                axes[i, 1].hist(df_minmax_scaled[col].dropna(), bins=30, alpha=0.7, color='red')\n",
    "                axes[i, 1].set_title(f'Min-Max Scaled: {col}')\n",
    "                axes[i, 1].set_xlabel('Scaled Value')\n",
    "                axes[i, 1].set_ylabel('Frequency')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No suitable numerical features found for Min-Max Scaling demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaling Implementation  \n",
    "# Based on LAB4 materials - scales features to mean=0, std=1\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_standard_scaling(df, numerical_columns):\n",
    "    \"\"\"\n",
    "    Apply Standard scaling (Z-score normalization) to numerical features\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with numerical features\n",
    "    numerical_columns: list of column names to scale\n",
    "    \n",
    "    Returns:\n",
    "    df_scaled: DataFrame with standardized features\n",
    "    scaler: fitted StandardScaler for inverse transform\n",
    "    \"\"\"\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    print(\"Applying Standard Scaling...\")\n",
    "    print(\"Standardizing numerical features to mean=0, std=1\")\n",
    "    \n",
    "    if numerical_columns:\n",
    "        # Initialize scaler\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        # Select only numerical columns that exist in dataframe\n",
    "        valid_columns = [col for col in numerical_columns if col in df_scaled.columns]\n",
    "        \n",
    "        if valid_columns:\n",
    "            # Apply scaling\n",
    "            df_scaled[valid_columns] = scaler.fit_transform(df_scaled[valid_columns])\n",
    "            \n",
    "            print(f\"Standardized {len(valid_columns)} numerical features:\")\n",
    "            for col in valid_columns[:5]:  # Show first 5 features\n",
    "                original_mean = df[col].mean()\n",
    "                original_std = df[col].std()\n",
    "                scaled_mean = df_scaled[col].mean()\n",
    "                scaled_std = df_scaled[col].std()\n",
    "                print(f\"  {col}: mean {original_mean:.2f}±{original_std:.2f} -> {scaled_mean:.2f}±{scaled_std:.2f}\")\n",
    "            \n",
    "            if len(valid_columns) > 5:\n",
    "                print(f\"  ... and {len(valid_columns) - 5} more features\")\n",
    "        else:\n",
    "            print(\"No valid numerical columns found for scaling\")\n",
    "            scaler = None\n",
    "    else:\n",
    "        print(\"No numerical columns provided for scaling\")\n",
    "        scaler = None\n",
    "    \n",
    "    return df_scaled, scaler\n",
    "\n",
    "# Compare scaling methods side by side\n",
    "if not df.empty and numerical_features:\n",
    "    # Select same features as Min-Max scaling for comparison\n",
    "    sample_numerical = []\n",
    "    for col in numerical_features[:10]:\n",
    "        if col in df.columns and not col.lower().endswith('_id'):\n",
    "            if df[col].nunique() > 2:\n",
    "                sample_numerical.append(col)\n",
    "    \n",
    "    if sample_numerical:\n",
    "        print(f\"Demonstrating Standard Scaling on: {len(sample_numerical)} features\")\n",
    "        df_standard_scaled, standard_scaler = apply_standard_scaling(df, sample_numerical)\n",
    "        \n",
    "        # Scaling comparison visualization\n",
    "        if len(sample_numerical) >= 1:\n",
    "            comparison_feature = sample_numerical[0]\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            # Original distribution\n",
    "            axes[0].hist(df[comparison_feature].dropna(), bins=30, alpha=0.7, color='blue')\n",
    "            axes[0].set_title(f'Original: {comparison_feature}')\n",
    "            axes[0].set_xlabel('Value')\n",
    "            axes[0].set_ylabel('Frequency')\n",
    "            \n",
    "            # Min-Max scaled\n",
    "            if 'df_minmax_scaled' in locals():\n",
    "                axes[1].hist(df_minmax_scaled[comparison_feature].dropna(), bins=30, alpha=0.7, color='red')\n",
    "                axes[1].set_title(f'Min-Max Scaled: {comparison_feature}')\n",
    "                axes[1].set_xlabel('Scaled Value [0,1]')\n",
    "                axes[1].set_ylabel('Frequency')\n",
    "            \n",
    "            # Standard scaled\n",
    "            axes[2].hist(df_standard_scaled[comparison_feature].dropna(), bins=30, alpha=0.7, color='green')\n",
    "            axes[2].set_title(f'Standard Scaled: {comparison_feature}')\n",
    "            axes[2].set_xlabel('Standardized Value')\n",
    "            axes[2].set_ylabel('Frequency')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Show scaling statistics\n",
    "            print(f\"\\nScaling Statistics for {comparison_feature}:\")\n",
    "            print(f\"Original: mean={df[comparison_feature].mean():.2f}, std={df[comparison_feature].std():.2f}\")\n",
    "            if 'df_minmax_scaled' in locals():\n",
    "                print(f\"Min-Max:  min={df_minmax_scaled[comparison_feature].min():.2f}, max={df_minmax_scaled[comparison_feature].max():.2f}\")\n",
    "            print(f\"Standard: mean={df_standard_scaled[comparison_feature].mean():.2f}, std={df_standard_scaled[comparison_feature].std():.2f}\")\n",
    "    else:\n",
    "        print(\"No suitable numerical features found for Standard Scaling demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding Implementation\n",
    "# Based on LAB4 materials - safe for linear models, avoids ordinal assumptions\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def apply_onehot_encoding(df, categorical_columns, drop_first=True, max_categories=10):\n",
    "    \"\"\"\n",
    "    Apply one-hot encoding to categorical features\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with categorical features  \n",
    "    categorical_columns: list of column names to encode\n",
    "    drop_first: whether to drop first category to avoid multicollinearity\n",
    "    max_categories: maximum unique values to consider for encoding\n",
    "    \n",
    "    Returns:\n",
    "    df_encoded: DataFrame with one-hot encoded features\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    print(\"Applying One-Hot Encoding...\")\n",
    "    for col in categorical_columns:\n",
    "        if col in df_encoded.columns:\n",
    "            # Check if feature has reasonable number of categories\n",
    "            n_categories = df_encoded[col].nunique()\n",
    "            \n",
    "            if n_categories <= max_categories:\n",
    "                # Apply one-hot encoding using pandas get_dummies\n",
    "                # This is more memory efficient than sklearn OneHotEncoder for small datasets\n",
    "                dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=drop_first)\n",
    "                \n",
    "                # Add dummy columns to dataframe\n",
    "                df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "                \n",
    "                print(f\"  {col}: {n_categories} categories -> {len(dummies.columns)} dummy columns\")\n",
    "            else:\n",
    "                print(f\"  {col}: Skipped ({n_categories} categories > {max_categories} threshold)\")\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Example usage with sample categorical features\n",
    "if not df.empty and categorical_features:\n",
    "    # Select categorical features with reasonable number of categories\n",
    "    suitable_features = []\n",
    "    for col in categorical_features[:3]:\n",
    "        if col in df.columns and df[col].nunique() <= 10:\n",
    "            suitable_features.append(col)\n",
    "    \n",
    "    if suitable_features:\n",
    "        print(f\"Demonstrating One-Hot Encoding on: {suitable_features}\")\n",
    "        df_onehot_encoded = apply_onehot_encoding(df, suitable_features)\n",
    "        \n",
    "        # Show the new dummy columns created\n",
    "        for col in suitable_features:\n",
    "            dummy_cols = [c for c in df_onehot_encoded.columns if c.startswith(col + '_')]\n",
    "            if dummy_cols:\n",
    "                print(f\"\\n{col} dummy columns: {dummy_cols}\")\n",
    "                print(f\"Sample values:\")\n",
    "                print(df_onehot_encoded[dummy_cols].head())\n",
    "    else:\n",
    "        print(\"No suitable categorical features found for One-Hot Encoding demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding Implementation\n",
    "# Based on LAB4 materials - simple encoding for binary/ordinal categorical variables\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def apply_label_encoding(df, categorical_columns):\n",
    "    \"\"\"\n",
    "    Apply label encoding to categorical features\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with categorical features\n",
    "    categorical_columns: list of column names to encode\n",
    "    \n",
    "    Returns:\n",
    "    df_encoded: DataFrame with label encoded features\n",
    "    encoders: dict of fitted encoders for inverse transform\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    encoders = {}\n",
    "    \n",
    "    print(\"Applying Label Encoding...\")\n",
    "    for col in categorical_columns:\n",
    "        if col in df_encoded.columns:\n",
    "            # Create and fit encoder\n",
    "            le = LabelEncoder()\n",
    "            # Handle missing values by filling with 'Unknown'\n",
    "            df_encoded[col] = df_encoded[col].fillna('Unknown')\n",
    "            df_encoded[col + '_encoded'] = le.fit_transform(df_encoded[col])\n",
    "            \n",
    "            # Store encoder for later use\n",
    "            encoders[col] = le\n",
    "            \n",
    "            print(f\"  {col}: {len(le.classes_)} unique values -> 0 to {len(le.classes_)-1}\")\n",
    "    \n",
    "    return df_encoded, encoders\n",
    "\n",
    "# Example usage with sample categorical features\n",
    "if not df.empty and categorical_features:\n",
    "    # Select a few categorical features for demonstration\n",
    "    sample_categorical = categorical_features[:3] if len(categorical_features) >= 3 else categorical_features\n",
    "    \n",
    "    print(f\"Demonstrating Label Encoding on: {sample_categorical}\")\n",
    "    df_label_encoded, label_encoders = apply_label_encoding(df, sample_categorical)\n",
    "    \n",
    "    # Show before and after comparison\n",
    "    for col in sample_categorical:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n{col} - Original vs Encoded:\")\n",
    "            comparison = pd.DataFrame({\n",
    "                'Original': df[col].head(10),\n",
    "                'Encoded': df_label_encoded[col + '_encoded'].head(10)\n",
    "            })\n",
    "            print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Lending Club Data\n",
    "## COMP647 Assignment 03\n",
    "### Student ID: 1163127\n",
    "\n",
    "This notebook implements feature engineering techniques including:\n",
    "- Data loading and initial exploration\n",
    "- Categorical encoding methods\n",
    "- Feature scaling techniques\n",
    "\n",
    "Based on LAB4 materials and course teachings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data from Assignment 02\n",
    "try:\n",
    "    # Load the processed sample data\n",
    "    df = pd.read_csv('../data/processed/accepted_sample_10000.csv')\n",
    "    print(f\"Data loaded successfully: {df.shape}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Processed data not found. Please run Assignment 02 notebooks first.\")\n",
    "    # For demonstration, create sample data structure\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Display basic info about the dataset\n",
    "if not df.empty:\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nCategorical features: {len(categorical_features)}\")\n",
    "    print(f\"Numerical features: {len(numerical_features)}\")\n",
    "    \n",
    "    if categorical_features:\n",
    "        print(\"\\nSample categorical features:\")\n",
    "        for col in categorical_features[:5]:  # Show first 5\n",
    "            print(f\"  {col}: {df[col].nunique()} unique values\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
