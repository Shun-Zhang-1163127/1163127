{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline\n",
    "\n",
    "**COMP647 Assignment 02 - Student ID: 1163127**\n",
    "\n",
    "This notebook implements comprehensive data preprocessing for Lending Club loan data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "# System and utility libraries\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_data(sample_size='10000'):\n",
    "    \"\"\"\n",
    "    Load sample datasets for development and analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    sample_size (str): Size of sample to load ('1000', '10000', '50000')\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (accepted_df, rejected_df)\n",
    "    \"\"\"\n",
    "    # Define file paths for data loading\n",
    "    data_path = '../data/processed/'\n",
    "    accepted_file = f'accepted_sample_{sample_size}.csv'\n",
    "    rejected_file = f'rejected_sample_{sample_size}.csv'\n",
    "    \n",
    "    print(f\"Loading sample datasets (size: {sample_size})...\")\n",
    "    \n",
    "    try:\n",
    "        # Load accepted loans dataset\n",
    "        accepted_df = pd.read_csv(os.path.join(data_path, accepted_file))\n",
    "        print(f\"Accepted loans loaded: {accepted_df.shape[0]:,} rows, {accepted_df.shape[1]} columns\")\n",
    "        \n",
    "        # Load rejected loans dataset\n",
    "        rejected_df = pd.read_csv(os.path.join(data_path, rejected_file))\n",
    "        print(f\"Rejected loans loaded: {rejected_df.shape[0]:,} rows, {rejected_df.shape[1]} columns\")\n",
    "        \n",
    "        print(\"Data loading completed successfully!\")\n",
    "        \n",
    "        return accepted_df, rejected_df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading files: {e}\")\n",
    "        print(\"Please ensure data files are in the correct directory\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during data loading: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of missing values in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    \n",
    "    Returns:\n",
    "    dict: Missing value analysis results\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing missing values for dataset with shape: {df.shape}\")\n",
    "    \n",
    "    # Calculate missing values for each column\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percentage = (missing_data / len(df)) * 100\n",
    "    \n",
    "    # Create detailed missing data summary\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Column': missing_data.index,\n",
    "        'Missing_Count': missing_data.values,\n",
    "        'Missing_Percentage': missing_percentage.values,\n",
    "        'Data_Type': df.dtypes.values,\n",
    "        'Unique_Values': [df[col].nunique() for col in df.columns]\n",
    "    })\n",
    "    \n",
    "    # Filter only columns with missing values\n",
    "    missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "    missing_summary = missing_summary.sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    total_missing = missing_data.sum()\n",
    "    columns_with_missing = len(missing_summary)\n",
    "    overall_missing_pct = (total_missing / (len(df) * len(df.columns))) * 100\n",
    "    \n",
    "    print(f\"Columns with missing values: {columns_with_missing}\")\n",
    "    print(f\"Total missing values: {total_missing:,}\")\n",
    "    print(f\"Overall missing percentage: {overall_missing_pct:.2f}%\")\n",
    "    \n",
    "    # Categorize missing values by severity\n",
    "    analysis_results = {\n",
    "        'missing_summary': missing_summary,\n",
    "        'total_missing_values': total_missing,\n",
    "        'columns_with_missing': columns_with_missing,\n",
    "        'overall_missing_percentage': overall_missing_pct,\n",
    "        'structural_missing_90plus': len(missing_summary[missing_summary['Missing_Percentage'] > 90]),\n",
    "        'high_missing_50_90': len(missing_summary[(missing_summary['Missing_Percentage'] > 50) & (missing_summary['Missing_Percentage'] <= 90)]),\n",
    "        'moderate_missing_10_50': len(missing_summary[(missing_summary['Missing_Percentage'] > 10) & (missing_summary['Missing_Percentage'] <= 50)]),\n",
    "        'low_missing_under_10': len(missing_summary[missing_summary['Missing_Percentage'] <= 10])\n",
    "    }\n",
    "    \n",
    "    if columns_with_missing > 0:\n",
    "        print(\"\\nMissing value categorization:\")\n",
    "        print(f\"  Structural missing (>90%): {analysis_results['structural_missing_90plus']} columns\")\n",
    "        print(f\"  High missing (50-90%): {analysis_results['high_missing_50_90']} columns\") \n",
    "        print(f\"  Moderate missing (10-50%): {analysis_results['moderate_missing_10_50']} columns\")\n",
    "        print(f\"  Low missing (<10%): {analysis_results['low_missing_under_10']} columns\")\n",
    "        \n",
    "        print(f\"\\nTop 10 columns with highest missing percentages:\")\n",
    "        if len(missing_summary) > 0:\n",
    "            for _, row in missing_summary.head(10).iterrows():\n",
    "                print(f\"  {row['Column']}: {row['Missing_Percentage']:.1f}% ({row['Missing_Count']:,} values)\")\n",
    "    else:\n",
    "        print(\"No missing values found in dataset\")\n",
    "    \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lending_data(df):\n",
    "    \"\"\"\n",
    "    Main preprocessing pipeline for Lending Club data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Raw lending data\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Preprocessed data\n",
    "    \"\"\"\n",
    "    print(f\"Starting preprocessing pipeline for dataset: {df.shape}\")\n",
    "    \n",
    "    # Step 1: Create a copy to avoid modifying original data\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Step 2: Basic data validation\n",
    "    print(\"Step 1: Basic data validation\")\n",
    "    print(f\"  Original shape: {df_processed.shape}\")\n",
    "    print(f\"  Memory usage: {df_processed.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Step 3: Remove duplicate rows\n",
    "    print(\"Step 2: Duplicate removal\")\n",
    "    initial_rows = len(df_processed)\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df_processed)\n",
    "    print(f\"  Duplicates removed: {duplicates_removed:,}\")\n",
    "    \n",
    "    # Step 4: Analyze missing values\n",
    "    print(\"Step 3: Missing value analysis\")\n",
    "    missing_analysis = analyze_missing_values(df_processed)\n",
    "    \n",
    "    # Step 5: Handle missing values based on analysis\n",
    "    print(\"Step 4: Missing value treatment\")\n",
    "    df_processed = handle_missing_values(df_processed, missing_analysis)\n",
    "    \n",
    "    # Step 6: Handle outliers if requested\n",
    "    print(\"Step 5: Outlier handling\")\n",
    "    numeric_columns = df_processed.select_dtypes(include=[np.number]).columns[:5]  # Process first 5 numeric columns for demo\n",
    "    if len(numeric_columns) > 0:\n",
    "        df_processed = handle_outliers(df_processed, columns=numeric_columns, method='iqr', action='cap')\n",
    "    else:\n",
    "        print(\"  No numeric columns found for outlier handling\")\n",
    "    \n",
    "    # Step 7: Data type optimization preparation\n",
    "    print(\"Step 6: Data type optimization\")\n",
    "    print(f\"  Current data types: {df_processed.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    print(f\"Preprocessing pipeline completed. Final shape: {df_processed.shape}\")\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Outlier Detection Functions\n",
    "\n",
    "Outlier detection using statistical methods for data quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, columns=None, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers using Interquartile Range (IQR) method\n",
    "    \n",
    "    This function identifies outliers by calculating the IQR for numeric columns\n",
    "    and flagging values that fall outside the bounds defined by Q1 - multiplier*IQR\n",
    "    and Q3 + multiplier*IQR.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataset for outlier analysis\n",
    "    columns (list): List of column names to analyze (None for all numeric columns)  \n",
    "    multiplier (float): IQR multiplier for outlier threshold (default: 1.5)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing outlier information for each analyzed column\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    outlier_info = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - multiplier * IQR\n",
    "            upper_bound = Q3 + multiplier * IQR\n",
    "            \n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            \n",
    "            outlier_info[col] = {\n",
    "                'count': len(outliers),\n",
    "                'percentage': (len(outliers) / len(df)) * 100,\n",
    "                'lower_bound': lower_bound,\n",
    "                'upper_bound': upper_bound,\n",
    "                'Q1': Q1,\n",
    "                'Q3': Q3,\n",
    "                'IQR': IQR\n",
    "            }\n",
    "    \n",
    "    return outlier_info\n",
    "\n",
    "def detect_outliers_zscore(df, columns=None, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using Z-score method\n",
    "    \n",
    "    This function identifies outliers using standardized Z-scores, flagging values\n",
    "    that have an absolute Z-score greater than the specified threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataset for outlier analysis\n",
    "    columns (list): List of column names to analyze (None for all numeric columns)\n",
    "    threshold (float): Z-score threshold for outlier detection (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing outlier information for each analyzed column\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    outlier_info = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "            outliers_mask = z_scores > threshold\n",
    "            outlier_count = outliers_mask.sum()\n",
    "            \n",
    "            outlier_info[col] = {\n",
    "                'count': outlier_count,\n",
    "                'percentage': (outlier_count / len(df[col].dropna())) * 100,\n",
    "                'threshold': threshold,\n",
    "                'max_zscore': z_scores.max() if len(z_scores) > 0 else 0\n",
    "            }\n",
    "    \n",
    "    return outlier_info\n",
    "\n",
    "print(\"Outlier detection functions defined successfully\")\n",
    "print(\"Available methods: detect_outliers_iqr(), detect_outliers_zscore()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Missing Value and Outlier Treatment\n",
    "\n",
    "Implementation of various missing value imputation and outlier handling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, missing_analysis, strategy='auto'):\n",
    "    \"\"\"\n",
    "    Handle missing values using appropriate imputation strategies\n",
    "    \n",
    "    This function applies different imputation methods based on missing value\n",
    "    percentage and data types. For columns with >90% missing data, it removes\n",
    "    them. For others, it applies median for numeric and mode for categorical.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to process\n",
    "    missing_analysis (dict): Results from analyze_missing_values function\n",
    "    strategy (str): Imputation strategy ('auto', 'mean', 'median', 'mode')\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Dataset with missing values handled\n",
    "    \"\"\"\n",
    "    print(f\"Starting missing value treatment with strategy: {strategy}\")\n",
    "    \n",
    "    df_treated = df.copy()\n",
    "    columns_dropped = []\n",
    "    columns_imputed = {}\n",
    "    \n",
    "    if missing_analysis['columns_with_missing'] == 0:\n",
    "        print(\"  No missing values to handle\")\n",
    "        return df_treated\n",
    "    \n",
    "    missing_summary = missing_analysis['missing_summary']\n",
    "    \n",
    "    for _, row in missing_summary.iterrows():\n",
    "        col_name = row['Column']\n",
    "        missing_pct = row['Missing_Percentage']\n",
    "        data_type = str(row['Data_Type'])\n",
    "        \n",
    "        # Drop columns with >90% missing values\n",
    "        if missing_pct > 90:\n",
    "            if col_name in df_treated.columns:\n",
    "                df_treated = df_treated.drop(columns=[col_name])\n",
    "                columns_dropped.append(col_name)\n",
    "                print(f\"  Dropped {col_name}: {missing_pct:.1f}% missing\")\n",
    "            continue\n",
    "        \n",
    "        # Handle numeric columns\n",
    "        if 'int' in data_type or 'float' in data_type:\n",
    "            if strategy == 'auto' or strategy == 'median':\n",
    "                fill_value = df_treated[col_name].median()\n",
    "                imputation_method = 'median'\n",
    "            elif strategy == 'mean':\n",
    "                fill_value = df_treated[col_name].mean()\n",
    "                imputation_method = 'mean'\n",
    "            \n",
    "            df_treated[col_name] = df_treated[col_name].fillna(fill_value)\n",
    "            columns_imputed[col_name] = imputation_method\n",
    "            print(f\"  Imputed {col_name}: {imputation_method} = {fill_value:.2f}\")\n",
    "        \n",
    "        # Handle categorical columns\n",
    "        else:\n",
    "            mode_value = df_treated[col_name].mode()\n",
    "            if len(mode_value) > 0:\n",
    "                fill_value = mode_value[0]\n",
    "                df_treated[col_name] = df_treated[col_name].fillna(fill_value)\n",
    "                columns_imputed[col_name] = 'mode'\n",
    "                print(f\"  Imputed {col_name}: mode = {fill_value}\")\n",
    "            else:\n",
    "                # Handle case where mode cannot be determined\n",
    "                df_treated[col_name] = df_treated[col_name].fillna('Unknown')\n",
    "                columns_imputed[col_name] = 'unknown'\n",
    "                print(f\"  Imputed {col_name}: fallback = Unknown\")\n",
    "    \n",
    "    print(f\"Missing value treatment completed:\")\n",
    "    print(f\"  Columns dropped: {len(columns_dropped)}\")\n",
    "    print(f\"  Columns imputed: {len(columns_imputed)}\")\n",
    "    print(f\"  Final shape: {df_treated.shape}\")\n",
    "    \n",
    "    return df_treated\n",
    "\n",
    "def handle_outliers(df, columns=None, method='iqr', action='cap'):\n",
    "    \"\"\"\n",
    "    Handle outliers in numeric columns using various methods\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to process\n",
    "    columns (list): Columns to process (None for all numeric)\n",
    "    method (str): Detection method ('iqr', 'zscore')\n",
    "    action (str): Action to take ('cap', 'remove')\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Dataset with outliers handled\n",
    "    \"\"\"\n",
    "    print(f\"Starting outlier handling: method={method}, action={action}\")\n",
    "    \n",
    "    df_treated = df.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df_treated.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    outliers_handled = 0\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df_treated.columns:\n",
    "            if method == 'iqr':\n",
    "                Q1 = df_treated[col].quantile(0.25)\n",
    "                Q3 = df_treated[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outlier_mask = (df_treated[col] < lower_bound) | (df_treated[col] > upper_bound)\n",
    "                \n",
    "            elif method == 'zscore':\n",
    "                z_scores = np.abs(stats.zscore(df_treated[col].dropna()))\n",
    "                outlier_mask = pd.Series([False] * len(df_treated))\n",
    "                valid_indices = df_treated[col].dropna().index\n",
    "                outlier_mask.loc[valid_indices] = z_scores > 3\n",
    "            \n",
    "            outlier_count = outlier_mask.sum()\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                if action == 'cap' and method == 'iqr':\n",
    "                    # Cap outliers to bounds\n",
    "                    df_treated.loc[df_treated[col] < lower_bound, col] = lower_bound\n",
    "                    df_treated.loc[df_treated[col] > upper_bound, col] = upper_bound\n",
    "                    print(f\"  Capped {outlier_count} outliers in {col}\")\n",
    "                \n",
    "                elif action == 'remove':\n",
    "                    # Remove outlier rows\n",
    "                    df_treated = df_treated[~outlier_mask]\n",
    "                    print(f\"  Removed {outlier_count} outlier rows for {col}\")\n",
    "                \n",
    "                outliers_handled += outlier_count\n",
    "    \n",
    "    print(f\"Outlier handling completed:\")\n",
    "    print(f\"  Total outliers processed: {outliers_handled}\")\n",
    "    print(f\"  Final shape: {df_treated.shape}\")\n",
    "    \n",
    "    return df_treated\n",
    "\n",
    "print(\"Missing value and outlier handling functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Execution\n",
    "\n",
    "This section demonstrates the complete preprocessing workflow using the implemented functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data for demonstration\n",
    "print(\"=== LOADING SAMPLE DATA ===\")\n",
    "df_accepted, df_rejected = load_sample_data(sample_size='10000')\n",
    "\n",
    "if df_accepted is not None and df_rejected is not None:\n",
    "    print(f\"\\nAccepted loans dataset shape: {df_accepted.shape}\")\n",
    "    print(f\"Rejected loans dataset shape: {df_rejected.shape}\")\n",
    "    \n",
    "    # Display basic info about accepted dataset\n",
    "    print(\"\\n=== ACCEPTED DATASET OVERVIEW ===\")\n",
    "    print(f\"Data types: {df_accepted.dtypes.value_counts().to_dict()}\")\n",
    "    print(f\"Memory usage: {df_accepted.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nFirst few columns:\")\n",
    "    print(df_accepted.columns[:10].tolist())\n",
    "else:\n",
    "    print(\"Failed to load data - check file paths and ensure sample files exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values in the accepted dataset\n",
    "if df_accepted is not None:\n",
    "    print(\"=== MISSING VALUE ANALYSIS ===\")\n",
    "    missing_results = analyze_missing_values(df_accepted)\n",
    "    \n",
    "    # Display detailed results\n",
    "    print(f\"\\nAnalysis Summary:\")\n",
    "    print(f\"- Total columns with missing data: {missing_results['columns_with_missing']}\")\n",
    "    print(f\"- Overall missing percentage: {missing_results['overall_missing_percentage']:.2f}%\")\n",
    "    \n",
    "    if missing_results['columns_with_missing'] > 0:\n",
    "        print(f\"\\nMissing data severity breakdown:\")\n",
    "        print(f\"- Structural missing (>90%): {missing_results['structural_missing_90plus']} columns\")\n",
    "        print(f\"- High missing (50-90%): {missing_results['high_missing_50_90']} columns\")\n",
    "        print(f\"- Moderate missing (10-50%): {missing_results['moderate_missing_10_50']} columns\") \n",
    "        print(f\"- Low missing (<10%): {missing_results['low_missing_under_10']} columns\")\n",
    "        \n",
    "        # Show the missing summary dataframe if it exists\n",
    "        if len(missing_results['missing_summary']) > 0:\n",
    "            print(f\"\\nTop 5 columns with highest missing percentages:\")\n",
    "            top_missing = missing_results['missing_summary'].head(5)\n",
    "            for _, row in top_missing.iterrows():\n",
    "                print(f\"- {row['Column']}: {row['Missing_Percentage']:.1f}% missing ({row['Data_Type']})\")\n",
    "else:\n",
    "    print(\"No data available for missing value analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete preprocessing pipeline\n",
    "if df_accepted is not None:\n",
    "    print(\"=== PREPROCESSING PIPELINE DEMONSTRATION ===\")\n",
    "    df_processed = preprocess_lending_data(df_accepted)\n",
    "    \n",
    "    print(f\"\\n=== PREPROCESSING RESULTS ===\")\n",
    "    print(f\"Original shape: {df_accepted.shape}\")\n",
    "    print(f\"Processed shape: {df_processed.shape}\")\n",
    "    \n",
    "    # Compare memory usage\n",
    "    original_memory = df_accepted.memory_usage(deep=True).sum() / 1024**2\n",
    "    processed_memory = df_processed.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Original memory usage: {original_memory:.2f} MB\")\n",
    "    print(f\"Processed memory usage: {processed_memory:.2f} MB\")\n",
    "    \n",
    "    if df_processed.shape[0] < df_accepted.shape[0]:\n",
    "        rows_removed = df_accepted.shape[0] - df_processed.shape[0]\n",
    "        print(f\"Rows removed during processing: {rows_removed:,}\")\n",
    "    \n",
    "    print(\"\\nPreprocessing pipeline demonstration completed!\")\n",
    "else:\n",
    "    print(\"No data available for preprocessing pipeline demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive preprocessing workflow demonstration\n",
    "if df_accepted is not None:\n",
    "    print(\"=== COMPREHENSIVE PREPROCESSING WORKFLOW ===\")\n",
    "    \n",
    "    # Step 1: Initial data assessment\n",
    "    print(f\"\\nStep 1: Initial Assessment\")\n",
    "    print(f\"Dataset shape: {df_accepted.shape}\")\n",
    "    print(f\"Memory usage: {df_accepted.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Step 2: Run complete preprocessing pipeline\n",
    "    print(f\"\\nStep 2: Complete Preprocessing Pipeline\")\n",
    "    df_fully_processed = preprocess_lending_data(df_accepted)\n",
    "    \n",
    "    # Step 3: Compare before and after\n",
    "    print(f\"\\n=== PREPROCESSING RESULTS SUMMARY ===\")\n",
    "    print(f\"Original dataset:\")\n",
    "    print(f\"  - Shape: {df_accepted.shape}\")\n",
    "    print(f\"  - Missing values: {df_accepted.isnull().sum().sum():,}\")\n",
    "    print(f\"  - Memory: {df_accepted.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nProcessed dataset:\")\n",
    "    print(f\"  - Shape: {df_fully_processed.shape}\")\n",
    "    print(f\"  - Missing values: {df_fully_processed.isnull().sum().sum():,}\")\n",
    "    print(f\"  - Memory: {df_fully_processed.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Step 4: Calculate improvement metrics\n",
    "    rows_change = df_fully_processed.shape[0] - df_accepted.shape[0]\n",
    "    cols_change = df_fully_processed.shape[1] - df_accepted.shape[1]\n",
    "    missing_reduction = df_accepted.isnull().sum().sum() - df_fully_processed.isnull().sum().sum()\n",
    "    \n",
    "    print(f\"\\nPreprocessing Impact:\")\n",
    "    print(f\"  - Rows changed: {rows_change:,} ({rows_change/df_accepted.shape[0]*100:.1f}%)\")\n",
    "    print(f\"  - Columns changed: {cols_change:,}\")\n",
    "    print(f\"  - Missing values eliminated: {missing_reduction:,}\")\n",
    "    \n",
    "    # Step 5: Data quality assessment\n",
    "    print(f\"\\nData Quality Assessment:\")\n",
    "    completeness = (1 - df_fully_processed.isnull().sum().sum() / (df_fully_processed.shape[0] * df_fully_processed.shape[1])) * 100\n",
    "    print(f\"  - Data completeness: {completeness:.2f}%\")\n",
    "    print(f\"  - Ready for analysis: {'Yes' if completeness > 95 else 'Needs review'}\")\n",
    "    \n",
    "    print(\"\\n=== PREPROCESSING WORKFLOW COMPLETED SUCCESSFULLY ===\")\n",
    "    print(\"Dataset is now ready for exploratory data analysis and modeling\")\n",
    "else:\n",
    "    print(\"No data available for comprehensive preprocessing demonstration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}